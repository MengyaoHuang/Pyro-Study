{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Poutine: A Guide to Programming with Effect Handlers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0OpAwphdqF4",
        "colab_type": "text"
      },
      "source": [
        "## Poutine: A Guide to Programming with Effect Handlers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxGQKSZ3d-ex",
        "colab_type": "code",
        "outputId": "3f80983a-6f10-469e-e168-700f90e12618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "!pip install pyro-ppl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyro-ppl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/0e/0523cb040c8f3ee8644b4280f6a72ed598ac7864680b667d6052fb5d445a/pyro-ppl-0.3.4.tar.gz (262kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.5.5)\n",
            "Requirement already satisfied: graphviz>=0.8 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.16.4)\n",
            "Collecting opt_einsum>=2.3.2 (from pyro-ppl)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.12.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.1.0)\n",
            "Collecting tqdm>=4.31 (from pyro-ppl)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/3d/7a6b68b631d2ab54975f3a4863f3c4e9b26445353264ef01f465dc9b0208/tqdm-4.32.2-py2.py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 19.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyro-ppl, opt-einsum\n",
            "  Building wheel for pyro-ppl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyro-ppl: filename=pyro_ppl-0.3.4-cp36-none-any.whl size=365502 sha256=4299f40936407c7a898e502b36e46c398baf92655150aa5b8108b95a3f6e2220\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/de/b5/88300d2adc973a7ec963b339d2935d34a0cf02c08b613a8a5e\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opt-einsum: filename=opt_einsum-2.3.2-cp36-none-any.whl size=49882 sha256=dae15c75ea89ac421dd833d7ed9daa1152ac2987c95a4a2550596b8f36b80973\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
            "Successfully built pyro-ppl opt-einsum\n",
            "Installing collected packages: opt-einsum, tqdm, pyro-ppl\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed opt-einsum-2.3.2 pyro-ppl-0.3.4 tqdm-4.32.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnSBJFZydsFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "import pyro.poutine as poutine\n",
        "\n",
        "from pyro.poutine.runtime import effectful\n",
        "\n",
        "pyro.set_rng_seed(101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6bBZcxpl6RN",
        "colab_type": "text"
      },
      "source": [
        "### A first example for joint probability distribution inference\n",
        "\n",
        "- This model below defines a joint probability distribution over \"weight\" and \"measurement\":\n",
        "  - weight|guess ~ Normal(guess, 1)\n",
        "  - measurement|guess|weight ~ Normal(weight, 0.75)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55JMpgPOeEjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scale(guess):\n",
        "    weight = pyro.sample(\"weight\", dist.Normal(guess, 1.0))\n",
        "    return pyro.sample(\"measurement\", dist.Normal(weight, 0.75))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqxybkUhnJu1",
        "colab_type": "text"
      },
      "source": [
        "- If we had access to the inputs and outputs of each pyro.sample site, we could compute their log-joint:\n",
        "```\n",
        "# This is formatted as code\n",
        "logp = dist.Normal(guess, 1.0).log_prob(weight).sum() + dist.Normal(weight, 0.75).log_prob(measurement).sum()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm6cuKJrn4yt",
        "colab_type": "text"
      },
      "source": [
        "### A first look at Poutine: Pyro’s library of algorithmic building blocks\n",
        "\n",
        "- Poutine is an Effect handlers library provided in Pyro\n",
        "- Compose two existing effect handlers first:\n",
        "  - poutine.condition: sets output values of pyro.sample statements\n",
        "  - poutine.trace: records the inputs, distributions, and outputs of pyro.sample statements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saY1C7SmsP6b",
        "colab_type": "text"
      },
      "source": [
        "- conditionMessenger class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WYilsP2sTaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Do NOT run this chunk - just for reference\n",
        "\"\"\"\n",
        "\n",
        "# Adds values at observe sites to condition on data and override sampling\n",
        "class ConditionMessenger(Messenger):\n",
        "    def __init__(self, data):\n",
        "        # data here would be a dictionary or a Trace\n",
        "        super(ConditionMessenger, self).__init__()\n",
        "        self.data = data\n",
        "\n",
        "    def _pyro_sample(self, msg):\n",
        "        # msg here would be current message at a trace site\n",
        "        # returns a sample from the stochastic function at the site\n",
        "\n",
        "        # If msg[\"name\"] appears in self.data, \n",
        "        # convert the sample site into an observe site\n",
        "        # whose observed value is the value from self.data[msg[\"name\"]].\n",
        "        # Otherwise, implements default sampling behavior\n",
        "        # with no additional effects.\n",
        "\n",
        "        name = msg[\"name\"]\n",
        "        if name in self.data:\n",
        "            assert not msg[\"is_observed\"], \\\n",
        "                \"should not change values of existing observes\"\n",
        "            if isinstance(self.data, Trace):\n",
        "                msg[\"value\"] = self.data.nodes[name][\"value\"]\n",
        "            else:\n",
        "                msg[\"value\"] = self.data[name]\n",
        "            msg[\"is_observed\"] = True\n",
        "        return None\n",
        "      \n",
        "    def _pyro_param(self, msg):\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QPS1bc9eEdY",
        "colab_type": "code",
        "outputId": "110a4447-e772-4e0b-e9fe-10beadda53c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "def make_log_joint(model):\n",
        "    def _log_joint(cond_data, *args, **kwargs):\n",
        "        conditioned_model = poutine.condition(model, data=cond_data)\n",
        "        trace = poutine.trace(conditioned_model).get_trace(*args, **kwargs)\n",
        "        return trace.log_prob_sum()\n",
        "    return _log_joint\n",
        "\n",
        "scale_log_joint = make_log_joint(scale)\n",
        "print(scale_log_joint({\"measurement\": 9.5, \"weight\": 8.23}, 8.5))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-3.0203)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecObSX4_qs_-",
        "colab_type": "code",
        "outputId": "6d58618f-3ea0-4a60-f769-72a9a90023b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# Further explanations for chunk above\n",
        "\n",
        "# Poutine.trace and poutine.condition are wrappers for context managers \n",
        "# that presumably communicate with the model through something inside pyro.sample\n",
        "\n",
        "# Poutine.trace produces a data structure (a Trace) containing a dictionary \n",
        "# whose keys are sample site names and values are dictionaries \n",
        "# containing the distribution (\"fn\") and output (\"value\") at each site\n",
        "\n",
        "# Again to reminder: \n",
        "# A Messenger is placed at the bottom of the stack when its enter method is called, \n",
        "# i.e. when it is used in a “with” statement\n",
        "\n",
        "\n",
        "from pyro.poutine.trace_messenger import TraceMessenger\n",
        "from pyro.poutine.condition_messenger import ConditionMessenger\n",
        "\n",
        "def make_log_joint_2(model):\n",
        "    def _log_joint(cond_data, *args, **kwargs):\n",
        "        with TraceMessenger() as tracer:\n",
        "            with ConditionMessenger(data=cond_data):\n",
        "                # here sample \"weight\" and \"measurement\" in model \"scale\"\n",
        "                model(*args, **kwargs)\n",
        "\n",
        "        trace = tracer.trace\n",
        "        logp = 0.\n",
        "        # here trace records all msg regardless of type\n",
        "        for name, node in trace.nodes.items():\n",
        "            if node[\"type\"] == \"sample\":\n",
        "                if node[\"is_observed\"]:\n",
        "                    assert node[\"value\"] is cond_data[name]\n",
        "                logp = logp + node[\"fn\"].log_prob(node[\"value\"]).sum()\n",
        "        return logp\n",
        "    return _log_joint\n",
        "\n",
        "scale_log_joint = make_log_joint_2(scale)\n",
        "\n",
        "# here 8.5 below is the input for guess in model \"scale\"\n",
        "# dictionary is the input for conditionMessenger\n",
        "\n",
        "# explanation: if our cond_data provides all values then use given values\n",
        "# to calculate the log_probs. Otherwise, we would follow the sampled values before - conditionMessenger operates\n",
        "print(scale_log_joint({}, 8.5))\n",
        "print(scale_log_joint({\"measurement\": 9.5}, 8.5))\n",
        "scale_log_joint({\"measurement\": 9.5, \"weight\": 8.23}, 8.5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-2.8493)\n",
            "tensor(-3.1514)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-3.0203)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nifipXx0MmM",
        "colab_type": "text"
      },
      "source": [
        "### Implementing new effect handlers with the Messenger API - a more complicated user-defined Messenger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9lXn-m-8gdq",
        "colab_type": "code",
        "outputId": "946a6e32-8165-49df-a460-7c9a7f164055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "# __init__ and __call__ in python\n",
        "class A:\n",
        "  def __init__(self):\n",
        "    print(\"init\")\n",
        "  def __call__(self):\n",
        "    print(\"call\")\n",
        "    \n",
        "# happen during initialization\n",
        "a = A()\n",
        "# happen when the class is called\n",
        "a()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init\n",
            "call\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rh0qqil_5XZ",
        "colab_type": "text"
      },
      "source": [
        "### A tip for using clone() below in the LogJointMessenger\n",
        "\n",
        "- tensor.detach() creates a tensor that shares storage with tensor that does not require grad. \n",
        "- tensor.clone()creates a copy of tensor that imitates the original tensor's requires_grad field.\n",
        "- use detach() when attempting to remove a tensor from a computation graph, and clone as a way to copy the tensor while still keeping the copy as a part of the computation graph it came from.\n",
        "- tensor.data returns a new tensor that shares storage with tensor. However, it always has requires_grad=False (even if the original tensor had requires_grad=True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaB7U_Fpqs77",
        "colab_type": "code",
        "outputId": "16873d76-d83b-4f53-f284-03fe48f5b84d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "class LogJointMessenger(poutine.messenger.Messenger):\n",
        "\n",
        "    def __init__(self, cond_data):\n",
        "        self.data = cond_data\n",
        "\n",
        "    # __call__ is syntactic sugar for using Messengers as higher-order functions.\n",
        "    # Messenger already defines __call__, but we re-define it here\n",
        "    # for exposition and to change the return value:\n",
        "    def __call__(self, fn):\n",
        "        def _fn(*args, **kwargs):\n",
        "            # this with command would call the messenger itself\n",
        "            # to handle messages from running \"fn\"\n",
        "            with self:\n",
        "                fn(*args, **kwargs)\n",
        "                # return self.logp\n",
        "                return self.logp.clone()\n",
        "        return _fn\n",
        "    \n",
        "    # Always override __enter__ and __exit__ when using new Messenger!\n",
        "    \n",
        "    def __enter__(self):\n",
        "        self.logp = torch.tensor(0.)\n",
        "        # All Messenger subclasses must call the base Messenger.__enter__()\n",
        "        # in their __enter__ methods\n",
        "        # __enter__ would push Messenger itself to the bottom of the stack\n",
        "        return super(LogJointMessenger, self).__enter__()\n",
        "\n",
        "    # __exit__ takes the same arguments in all Python context managers\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.logp = torch.tensor(0.)\n",
        "        # All Messenger subclasses must call the base Messenger.__exit__ method\n",
        "        # in their __exit__ methods.\n",
        "        return super(LogJointMessenger, self).__exit__(exc_type, exc_value, traceback)\n",
        "\n",
        "    # _pyro_sample will be called once per pyro.sample site.\n",
        "    # It takes a dictionary msg containing the name, distribution,\n",
        "    # observation or sample value, and other metadata from the sample site.\n",
        "    # work as __process__messsage__ in class Messenger\n",
        "    def _pyro_sample(self, msg):\n",
        "        assert msg[\"name\"] in self.data\n",
        "        msg[\"value\"] = self.data[msg[\"name\"]]\n",
        "        # Since we've observed a value for this site, we set the \"is_observed\" flag to True\n",
        "        # This tells any other Messengers not to overwrite msg[\"value\"] with a sample.\n",
        "        msg[\"is_observed\"] = True\n",
        "        # \"scale\": torch.tensor(1.) - a key in msg dic\n",
        "        # Multiplicative scale factor that can be applied to each site's log_prob\n",
        "        self.logp = self.logp + (msg[\"scale\"] * msg[\"fn\"].log_prob(msg[\"value\"])).sum()\n",
        "\n",
        "        \n",
        "# add the LogJointMessenger into the handler stack to process all messages generated during model \"scale\"\n",
        "with LogJointMessenger(cond_data={\"measurement\": 9.5, \"weight\": 8.23}) as m:\n",
        "    scale(8.5)\n",
        "    # print(m.logp)\n",
        "    print(m.logp.clone())\n",
        "\n",
        "scale_log_joint = LogJointMessenger(cond_data={\"measurement\": 9.5, \"weight\": 8.23})(scale)\n",
        "print(scale_log_joint(8.5))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-3.0203)\n",
            "tensor(-3.0203)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9PdPgYIqs5D",
        "colab_type": "code",
        "outputId": "dd6b989d-1182-4db0-9da9-a5fcae630c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# A common way to use LogJointMessenger as a context wrapper with in a function\n",
        "def log_joint(model=None, cond_data=None):\n",
        "    msngr = LogJointMessenger(cond_data=cond_data)\n",
        "    return msngr(model) if model is not None else msngr\n",
        "\n",
        "# All msgs generated in model \"scale\" must be in keys provided in cond_data\n",
        "scale_log_joint = log_joint(scale, cond_data={\"measurement\": 9.5, \"weight\": 8.23})\n",
        "print(scale_log_joint(8.5))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-3.0203)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSmGX_QTAtZh",
        "colab_type": "text"
      },
      "source": [
        "### Extension to the LogJointMessenger example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gNza2ZpA7Ax",
        "colab_type": "code",
        "outputId": "6f7fd91e-dcdd-4471-bcf6-4e452356b07d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "class LogJointMessenger2(poutine.messenger.Messenger):\n",
        "  \n",
        "    def __init__(self, cond_data):\n",
        "        self.data = cond_data\n",
        "    def __call__(self, fn):\n",
        "        def _fn(*args, **kwargs):\n",
        "            with self:\n",
        "                fn(*args, **kwargs)\n",
        "                return self.logp.clone()\n",
        "        return _fn\n",
        "    def __enter__(self):\n",
        "        self.logp = torch.tensor(0.)\n",
        "        return super(LogJointMessenger2, self).__enter__()\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.logp = torch.tensor(0.)\n",
        "        return super(LogJointMessenger2, self).__exit__(exc_type, exc_value, traceback)  \n",
        "      \n",
        "    def _pyro_sample(self, msg):\n",
        "        if msg[\"name\"] in self.data:\n",
        "            msg[\"value\"] = self.data[msg[\"name\"]]\n",
        "            msg[\"done\"] = True\n",
        "    \n",
        "    # necessary because some effects can only be applied \n",
        "    # after all other effect handlers have had a chance to update the message once\n",
        "    def _pyro_post_sample(self, msg):\n",
        "        assert msg[\"done\"]  # the \"done\" flag asserts that no more modifications to value and fn will be performed.\n",
        "        print(msg[\"name\"])\n",
        "        self.logp = self.logp + (msg[\"scale\"] * msg[\"fn\"].log_prob(msg[\"value\"])).sum()\n",
        "        \n",
        "with LogJointMessenger2(cond_data={\"measurement\": 9.5}) as m:\n",
        "    # Here weight is not the cond_data dic but its msg[\"done\"] is True\n",
        "    # after being operated by other Messengers or default when stack is empty?\n",
        "    scale(8.5)\n",
        "    print(m.logp)\n",
        "\n",
        "with LogJointMessenger2(cond_data={\"measurement\": 9.5, \"weight\": 8.23}) as m:\n",
        "    scale(8.5)\n",
        "    print(m.logp)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weight\n",
            "measurement\n",
            "tensor(-1.8835)\n",
            "weight\n",
            "measurement\n",
            "tensor(-3.0203)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycgefNZMGUvI",
        "colab_type": "text"
      },
      "source": [
        "### Inside the messages sent by Messengers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiN4AXOaAdXZ",
        "colab_type": "code",
        "outputId": "f543fdff-3833-4dfe-e031-2972a9148ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "msg = {\n",
        "    # The following fields contain the name, inputs, function, and output of a site.\n",
        "    # These are generally the only fields you'll need to think about.\n",
        "    \"name\": \"x\",\n",
        "    \"fn\": dist.Bernoulli(0.5),\n",
        "    \"value\": None,  # msg[\"value\"] will eventually contain the value returned by pyro.sample\n",
        "    \"is_observed\": False,  # because obs=None by default; only used by sample sites\n",
        "    \n",
        "    \"args\": (),  # positional arguments passed to \"fn\" when it is called; usually empty for sample sites\n",
        "    \"kwargs\": {},  # keyword arguments passed to \"fn\" when it is called; usually empty for sample sites\n",
        "    \n",
        "    # This field typically contains metadata needed or stored by a particular inference algorithm\n",
        "    \"infer\": {\"enumerate\": \"parallel\"},\n",
        "    \n",
        "    # The remaining fields are generally only used by Pyro's internals,\n",
        "    # or for implementing more advanced effects beyond the scope of this tutorial\n",
        "    \"type\": \"sample\",  # label used by Messenger._process_message to dispatch, in this case to _pyro_sample\n",
        "    \"done\": False,\n",
        "    \"stop\": False,\n",
        "    \"scale\": torch.tensor(1.),  # Multiplicative scale factor that can be applied to each site's log_prob\n",
        "    \"mask\": None,\n",
        "    \"continuation\": None,\n",
        "    \"cond_indep_stack\": (),  # Will contain metadata from each pyro.plate enclosing this sample site.\n",
        "}\n",
        "pyro.sample(\"x\", dist.Bernoulli(0.5), infer={\"enumerate\": \"parallel\"}, obs=None)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gnmd79fUaqS",
        "colab_type": "text"
      },
      "source": [
        "## Implementing inference algorithms with existing effect handlers: examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT-mZVOpUl05",
        "colab_type": "text"
      },
      "source": [
        "### Example1: Variational inference with a Monte Carlo ELBO\n",
        "\n",
        "- ELBO training attached in the Mini Pyro page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntq93LrPAdU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def monte_carlo_elbo(model, guide, batch, *args, **kwargs):\n",
        "    # assuming batch is a dictionary, we use poutine.condition to fix values of observed variables\n",
        "    conditioned_model = poutine.condition(model, data=batch)\n",
        "\n",
        "    # we'll approximate the expectation in the ELBO with a single sample:\n",
        "    # first, we run the guide forward unmodified and record values and distributions\n",
        "    # at each sample site using poutine.trace\n",
        "    guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n",
        "\n",
        "    # we use poutine.replay to set the values of latent variables in the model\n",
        "    # to the values sampled above by our guide, and use poutine.trace\n",
        "    # to record the distributions that appear at each sample site in in the model\n",
        "    model_trace = poutine.trace(poutine.replay(conditioned_model, \n",
        "                                               trace=guide_trace)).get_trace(*args, **kwargs)\n",
        "    \n",
        "    elbo = 0.\n",
        "    for name, node in model_trace.nodes.items():\n",
        "        if node[\"type\"] == \"sample\":\n",
        "            elbo = elbo + node[\"fn\"].log_prob(node[\"value\"]).sum()\n",
        "            if not node[\"is_observed\"]:\n",
        "                elbo = elbo - guide_trace.nodes[name][\"fn\"].log_prob(node[\"value\"]).sum()\n",
        "    return -elbo\n",
        "\n",
        "  \n",
        "# use poutine.trace and poutine.block to record pyro.param calls for optimization\n",
        "def train(model, guide, data):\n",
        "    optimizer = pyro.optim.Adam({})\n",
        "    for batch in data:\n",
        "        # this poutine.trace will record all of the parameters that appear in the model and guide\n",
        "        # during the execution of monte_carlo_elbo\n",
        "        with poutine.trace() as param_capture:\n",
        "            # we use poutine.block here so that only parameters appear in the trace above\n",
        "            with poutine.block(hide_fn=lambda node: node[\"type\"] != \"param\"):\n",
        "                loss = monte_carlo_elbo(model, guide, batch)\n",
        "\n",
        "        loss.backward()\n",
        "        params = set(node[\"value\"].unconstrained()\n",
        "                     for node in param_capture.trace.nodes.values())\n",
        "        optimizer.step(params)\n",
        "        pyro.infer.util.zero_grads(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTpe1g3qVxxh",
        "colab_type": "text"
      },
      "source": [
        "### Example2: Exact inference via sequential enumeration\n",
        "\n",
        "- This example uses poutine.queue, itself implemented using poutine.trace, poutine.replay, and poutine.block, to enumerate over possible values of all discrete variables in a model and compute a marginal distribution over all possible return values or the possible values at a particular sample site."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Rh18wJImXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Used by poutine.EscapeMessenger to return site information\n",
        "# reset_stack()\n",
        "# Reset the state of the frames remaining in the stack. Necessary for multiple re-executions in poutine.queue\n",
        "\n",
        "# am_i_wrapped()\n",
        "# Checks whether the current computation is wrapped in a poutine. :returns: bool\n",
        "\n",
        "# apply_stack(initial_msg)\n",
        "# Execute the effect stack at a single site\n",
        "\n",
        "class NonlocalExit(Exception):\n",
        "    def __init__(self, site, *args, **kwargs):\n",
        "        super(NonlocalExit, self).__init__(*args, **kwargs)\n",
        "        self.site = site\n",
        "        \n",
        "# Messenger that does a nonlocal exit by raising a util.NonlocalExit exception\n",
        "class EscapeMessenger(Messenger):\n",
        "  \n",
        "    def __init__(self, escape_fn):\n",
        "        # escape_fn: function that takes a msg as input and returns True\n",
        "        # if the poutine should perform a nonlocal exit at that site.\n",
        "        super(EscapeMessenger, self).__init__()\n",
        "        self.escape_fn = escape_fn\n",
        "\n",
        "    def _pyro_sample(self, msg):\n",
        "        # returns a sample from the stochastic function at the site.\n",
        "        # Evaluates self.escape_fn on the site (self.escape_fn(msg)).\n",
        "        # If this returns True, raises an exception NonlocalExit(msg).\n",
        "        # Else, implements default _pyro_sample behavior with no additional effects.\n",
        "        if self.escape_fn(msg):\n",
        "            msg[\"done\"] = True\n",
        "            msg[\"stop\"] = True\n",
        "\n",
        "            def cont(m):\n",
        "                raise NonlocalExit(m)\n",
        "            msg[\"continuation\"] = cont\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOztRTKHLH4k",
        "colab_type": "code",
        "outputId": "ae7b47da-1aaf-4212-ca0a-bc859cd3d544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import queue\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.poutine.trace_messenger import TraceMessenger\n",
        "from pyro.poutine.condition_messenger import ConditionMessenger\n",
        "\n",
        "# Some preparation\n",
        "\n",
        "# Initialize queue\n",
        "temp = queue.Queue(10)\n",
        "# Insert Element\n",
        "temp.put(2)\n",
        "# Get And remove the element\n",
        "print(temp.get())\n",
        "\n",
        "# functool.partial\n",
        "# keep part of args and keywords and extend any updates\n",
        "def partial(func, *args, **keywords):\n",
        "  \n",
        "    def newfunc(*fargs, **fkeywords):\n",
        "        newkeywords = keywords.copy()\n",
        "        newkeywords.update(fkeywords)\n",
        "        return func(*(args + fargs), **newkeywords)\n",
        "      \n",
        "    newfunc.func = func\n",
        "    newfunc.args = args\n",
        "    newfunc.keywords = keywords\n",
        "    return newfunc\n",
        "\n",
        "# replay(fn=None, trace=None, params=None)\n",
        "# Given a callable that contains Pyro primitive calls, return a callable that runs the original, \n",
        "# reusing the values at sites in trace at those sites in the new trace\n",
        "def model(x):\n",
        "  s = pyro.param(\"s\", torch.tensor(0.5))\n",
        "  z = pyro.sample(\"z\", dist.Normal(x, s))\n",
        "  return z ** 2\n",
        "\n",
        "old_trace = poutine.trace(model).get_trace(1.0)\n",
        "replayed_model = poutine.replay(model, trace=old_trace)\n",
        "print(replayed_model(0.0))\n",
        "bool(replayed_model(0.0) == old_trace.nodes[\"_RETURN\"][\"value\"])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "tensor(1.9962, grad_fn=<PowBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS57siN3E9vU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# enumerate over possible values of all discrete variables in a model \n",
        "# and compute a marginal distribution over all possible return values or the possible values at a particular sample site\n",
        "\n",
        "# poutine.queue function for sequential enumeration over discrete variables\n",
        "# Given a stochastic function and a queue,\n",
        "# return a return value from a complete trace in the queue.\n",
        "\n",
        "def queue(fn=None, queue=None, max_tries=None,\n",
        "          extend_fn=None, escape_fn=None, num_samples=None):\n",
        "    \"\"\"\n",
        "    :param fn: a stochastic function (callable containing Pyro primitive calls)\n",
        "    :param queue: a queue data structure like multiprocessing.Queue to hold partial traces\n",
        "    :param max_tries: maximum number of attempts to compute a single complete trace\n",
        "    :param extend_fn: function (possibly stochastic) that takes a partial trace and a site,\n",
        "        and returns a list of extended traces\n",
        "    :param escape_fn: function (possibly stochastic) that takes a partial trace and a site,\n",
        "        and returns a boolean value to decide whether to exit\n",
        "    :param num_samples: optional number of extended traces for extend_fn to return\n",
        "    :returns: stochastic function decorated with poutine logic\n",
        "    \"\"\"\n",
        "    # initialization\n",
        "    if max_tries is None:\n",
        "        max_tries = int(1e6)\n",
        "    if extend_fn is None:\n",
        "        extend_fn = util.enum_extend\n",
        "    if escape_fn is None:\n",
        "        escape_fn = util.discrete_escape\n",
        "    if num_samples is None:\n",
        "        num_samples = -1\n",
        "\n",
        "    def wrapper(wrapped):\n",
        "        def _fn(*args, **kwargs):\n",
        "            for i in range(max_tries):\n",
        "                # get next trace from the queue\n",
        "                assert not queue.empty(), \\\n",
        "                    \"trying to get() from an empty queue will deadlock\"\n",
        "                next_trace = queue.get()\n",
        "                \n",
        "                try:\n",
        "                    ftr = trace(escape(replay(wrapped, trace=next_trace), escape_fn=functools.partial(escape_fn,next_trace)))\n",
        "                    return ftr(*args, **kwargs)\n",
        "                except NonlocalExit as site_container:\n",
        "                    site_container.reset_stack()\n",
        "                    for tr in extend_fn(ftr.trace.copy(),site_container.site,num_samples=num_samples):\n",
        "                        queue.put(tr)\n",
        "                        \n",
        "            raise ValueError(\"max tries ({}) exceeded\".format(str(max_tries)))\n",
        "        return _fn\n",
        "    return wrapper(fn) if fn is not None else wrapper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0TEnrhkU1Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sequential_discrete_marginal(model, data, site_name=\"_RETURN\"):\n",
        "\n",
        "    from six.moves import queue  # queue data structures\n",
        "    q = queue.Queue()  # Instantiate a first-in first-out queue\n",
        "    q.put(poutine.Trace())  # seed the queue with an empty trace\n",
        "\n",
        "    # as before, we fix the values of observed random variables with poutine.condition\n",
        "    # assuming data is a dictionary whose keys are names of sample sites in model\n",
        "    conditioned_model = poutine.condition(model, data=data)\n",
        "\n",
        "    # we wrap the conditioned model in a poutine.queue,\n",
        "    # which repeatedly pushes and pops partially completed executions from a Queue()\n",
        "    # to perform breadth-first enumeration over the set of values of all discrete sample sites in model\n",
        "    enum_model = poutine.queue(conditioned_model, queue=q)\n",
        "\n",
        "    # actually perform the enumeration by repeatedly tracing enum_model\n",
        "    # and accumulate samples and trace log-probabilities for postprocessing\n",
        "    samples, log_weights = [], []\n",
        "    while not q.empty():\n",
        "        # record the sample value and log weight at this trace site\n",
        "        trace = poutine.trace(enum_model).get_trace()\n",
        "        samples.append(trace.nodes[site_name][\"value\"])\n",
        "        log_weights.append(trace.log_prob_sum())\n",
        "\n",
        "    # we take the samples and log-joints and turn them into a histogram:\n",
        "    samples = torch.stack(samples, 0)\n",
        "    log_weights = torch.stack(log_weights, 0)\n",
        "    log_weights = log_weights - dist.util.logsumexp(log_weights, dim=0)\n",
        "    # Empirical distribution associated with the sampled data\n",
        "    return dist.Empirical(samples, log_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZygNWF_Dorx",
        "colab_type": "text"
      },
      "source": [
        "### Example3: implementing lazy evaluation with the Messenger API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLw6RtICUdAd",
        "colab_type": "code",
        "outputId": "c47a7dec-9db6-488f-cce0-323941dc8ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "class Foo:\n",
        "  a = 5\n",
        "fooInstance = Foo()\n",
        "\n",
        "print(isinstance(fooInstance, Foo))\n",
        "print(isinstance(fooInstance, (list, tuple)))\n",
        "print(isinstance(fooInstance, (list, tuple, Foo)))\n",
        "\n",
        "# isinstance(object, classinfo)\n",
        "# object - object to be checked\n",
        "# classinfo - class, type, or tuple of classes and types\n",
        "# True if the object is an instance or subclass of a class, or any element of the tuple\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG_UNkNMU1GZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first define a LazyValue class that we will use to build up a computation graph\n",
        "\n",
        "# With LazyValue, implementing lazy evaluation as a Messenger compatible with other effect handlers is suprisingly easy. \n",
        "# We just make each msg[\"value\"] a LazyValue and introduce a new operation type \"apply\" for deterministic operations\n",
        "class LazyValue(object):\n",
        "    def __init__(self, fn, *args, **kwargs):\n",
        "        self._expr = (fn, args, kwargs)\n",
        "        self._value = None\n",
        "        \n",
        "    def __str__(self):\n",
        "        return \"({} {})\".format(str(self._expr[0]), \" \".join(map(str, self._expr[1])))\n",
        "\n",
        "    def evaluate(self):\n",
        "        if self._value is None:\n",
        "            fn, args, kwargs = self._expr\n",
        "            \n",
        "            fn = fn.evaluate() if isinstance(fn, LazyValue) else fn\n",
        "            print(fn)\n",
        "            args = tuple(arg.evaluate() if isinstance(arg, LazyValue) else arg for arg in args)\n",
        "            print(args)\n",
        "            kwargs = {k: v.evaluate() if isinstance(v, LazyValue) else v for k, v in kwargs.items()}\n",
        "            print(kwargs.items())\n",
        "            \n",
        "            self._value = fn(*args, **kwargs)\n",
        "        return self._value\n",
        "\n",
        "class LazyMessenger(pyro.poutine.messenger.Messenger):\n",
        "    def _process_message(self, msg):\n",
        "        if msg[\"type\"] in (\"apply\", \"sample\") and not msg[\"done\"]:\n",
        "            print(\"Print from process message func: \")\n",
        "            print(msg[\"name\"])\n",
        "            print(msg[\"fn\"])\n",
        "            msg[\"done\"] = True\n",
        "            msg[\"value\"] = LazyValue(msg[\"fn\"], *msg[\"args\"], **msg[\"kwargs\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3CCmmJ0nQMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use pyro.poutine.runtime.effectful as a decorator to expose these operations to LazyMessenger. \n",
        "# effectful constructs a message much like the one above and sends it up and down the effect handler stack, \n",
        "# but allows us to set the type (in this case, to \"apply\" instead of \"sample\") \n",
        "# so that these operations aren’t mistaken for sample statements by other effect handlers like TraceMessenger\n",
        "\n",
        "@effectful(type=\"apply\")\n",
        "def add(x, y):\n",
        "    return x + y\n",
        "\n",
        "@effectful(type=\"apply\")\n",
        "def mul(x, y):\n",
        "    return x * y\n",
        "\n",
        "@effectful(type=\"apply\")\n",
        "def sigmoid(x):\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "@effectful(type=\"apply\")\n",
        "def normal(loc, scale):\n",
        "    return dist.Normal(loc, scale)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atMBwky2nQIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applied to another model\n",
        "def biased_scale(guess):\n",
        "    weight = pyro.sample(\"weight\", normal(guess, 1.))\n",
        "    tolerance = pyro.sample(\"tolerance\", normal(0., 0.25))\n",
        "    return pyro.sample(\"measurement\", normal(add(mul(weight, 0.8), 1.), sigmoid(tolerance)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI-dEMhHnQEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "outputId": "225b43c4-6b75-4190-bce8-5a81ccc28471"
      },
      "source": [
        "with LazyMessenger():\n",
        "    v = biased_scale(8.5)\n",
        "    print(v)\n",
        "    print(\"Now start v evaluate: \")\n",
        "    print(v.evaluate())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Print from process message func: \n",
            "None\n",
            "<function normal at 0x7fbe3777a620>\n",
            "Print from process message func: \n",
            "weight\n",
            "(<function normal at 0x7fbe3777a620> 8.5 1.0)\n",
            "Print from process message func: \n",
            "None\n",
            "<function normal at 0x7fbe3777a620>\n",
            "Print from process message func: \n",
            "tolerance\n",
            "(<function normal at 0x7fbe3777a620> 0.0 0.25)\n",
            "Print from process message func: \n",
            "None\n",
            "<function mul at 0x7fbe3777a400>\n",
            "Print from process message func: \n",
            "None\n",
            "<function add at 0x7fbe3777aa60>\n",
            "Print from process message func: \n",
            "None\n",
            "<function sigmoid at 0x7fbe3777a510>\n",
            "Print from process message func: \n",
            "None\n",
            "<function normal at 0x7fbe3777a620>\n",
            "Print from process message func: \n",
            "measurement\n",
            "(<function normal at 0x7fbe3777a620> (<function add at 0x7fbe3777aa60> (<function mul at 0x7fbe3777a400> ((<function normal at 0x7fbe3777a620> 8.5 1.0) ) 0.8) 1.0) (<function sigmoid at 0x7fbe3777a510> ((<function normal at 0x7fbe3777a620> 0.0 0.25) )))\n",
            "((<function normal at 0x7fbe3777a620> (<function add at 0x7fbe3777aa60> (<function mul at 0x7fbe3777a400> ((<function normal at 0x7fbe3777a620> 8.5 1.0) ) 0.8) 1.0) (<function sigmoid at 0x7fbe3777a510> ((<function normal at 0x7fbe3777a620> 0.0 0.25) ))) )\n",
            "Now start v evaluate: \n",
            "<function normal at 0x7fbe3777a620>\n",
            "<function add at 0x7fbe3777aa60>\n",
            "<function mul at 0x7fbe3777a400>\n",
            "<function normal at 0x7fbe3777a620>\n",
            "(8.5, 1.0)\n",
            "dict_items([])\n",
            "Normal(loc: 8.5, scale: 1.0)\n",
            "()\n",
            "dict_items([])\n",
            "(tensor(8.4762), 0.8)\n",
            "dict_items([])\n",
            "(tensor(6.7810), 1.0)\n",
            "dict_items([])\n",
            "<function sigmoid at 0x7fbe3777a510>\n",
            "<function normal at 0x7fbe3777a620>\n",
            "(0.0, 0.25)\n",
            "dict_items([])\n",
            "Normal(loc: 0.0, scale: 0.25)\n",
            "()\n",
            "dict_items([])\n",
            "(tensor(0.2969),)\n",
            "dict_items([])\n",
            "(tensor(7.7810), tensor(0.5737))\n",
            "dict_items([])\n",
            "Normal(loc: 7.780966281890869, scale: 0.5736812949180603)\n",
            "()\n",
            "dict_items([])\n",
            "tensor(7.6938)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7JG1D0v0D50",
        "colab_type": "text"
      },
      "source": [
        "### Some remaining points to test first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjldL35e0DYF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "3c1b01b4-bd90-44c0-f9a8-127774f896e7"
      },
      "source": [
        "# Pyro multivariate Bernoulli distribution \n",
        "\n",
        "# A first normal example using pyro distribution\n",
        "mu = 0; sigma = 1; normal = dist.Normal(mu, sigma); \n",
        "x = normal.sample(); \n",
        "#compute the log probability according to the distribution\n",
        "log_prob1 = normal.log_prob(x)\n",
        "print(\"sample\", x)\n",
        "print(\"log prob\",log_prob1)\n",
        "\n",
        "# Now take Bernoulli to have a try\n",
        "p = 0.5; bernoulli = dist.Bernoulli(p);\n",
        "y = bernoulli.sample()\n",
        "log_prob1 = normal.log_prob(y)\n",
        "print(\"sample\", y)\n",
        "print(\"log prob\",log_prob1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample tensor(-0.1651)\n",
            "log prob tensor(-0.9326)\n",
            "sample tensor(0.)\n",
            "log prob tensor(-0.9189)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1X8sHOX0Zzd",
        "colab_type": "text"
      },
      "source": [
        "### Tensor shapes in Pyro\n",
        "- Use .expand() to draw a batch of samples, or rely on plate to expand automatically.\n",
        "- Use my_dist.to_event(1) to declare a dimension as dependent.\n",
        "- Use with pyro.plate('name', size): to declare a dimension as conditionally independent.\n",
        "- All dimensions must be declared either dependent or conditionally independent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmkRAwLD0XD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import pyro\n",
        "from torch.distributions import constraints\n",
        "from pyro.distributions import Bernoulli, Categorical, MultivariateNormal, Normal\n",
        "from pyro.distributions.util import broadcast_shape\n",
        "from pyro.infer import Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
        "import pyro.poutine as poutine\n",
        "from pyro.optim import Adam\n",
        "\n",
        "smoke_test = ('CI' in os.environ)\n",
        "assert pyro.__version__.startswith('0.3.4')\n",
        "pyro.enable_validation(True)    # <---- This is always a good idea!\n",
        "\n",
        "# We'll ue this helper to check our models are correct.\n",
        "def test_model(model, guide, loss):\n",
        "    pyro.clear_param_store()\n",
        "    loss.loss(model, guide)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnluLxZm0mWW",
        "colab_type": "text"
      },
      "source": [
        "### Distributions shapes: batch_shape and event_shape\n",
        "- PyTorch Tensors have a single .shape attribute, but Distributions have two shape attributions with special meaning: .batch_shape and .event_shape. These two combine to define the total shape of a sample\n",
        "- http://docs.pyro.ai/en/0.2.1-release/_modules/pyro/distributions/torch.html#Bernoulli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaVK3ZW80W8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some distribution classes for reference\n",
        "import torch\n",
        "from pyro.distributions.torch_distribution import TorchDistributionMixin\n",
        "\n",
        "# e.g: for Bernoulli distribution, provide either probs or logits to sample\n",
        "class Bernoulli(torch.distributions.Bernoulli, TorchDistributionMixin):\n",
        "    def expand(self, batch_shape):\n",
        "        validate_args = self.__dict__.get('validate_args')\n",
        "        if 'probs' in self.__dict__:\n",
        "            probs = self.probs.expand(batch_shape)\n",
        "            return Bernoulli(probs=probs, validate_args=validate_args)\n",
        "        else:\n",
        "            logits = self.logits.expand(batch_shape)\n",
        "            return Bernoulli(logits=logits, validate_args=validate_args)\n",
        "\n",
        "class Beta(torch.distributions.Beta, TorchDistributionMixin):\n",
        "    def expand(self, batch_shape):\n",
        "        validate_args = self.__dict__.get('validate_args')\n",
        "        concentration1 = self.concentration1.expand(batch_shape)\n",
        "        concentration0 = self.concentration0.expand(batch_shape)\n",
        "        return Beta(concentration1, concentration0, validate_args=validate_args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX4ongZh0ypN",
        "colab_type": "text"
      },
      "source": [
        "- Indices over .batch_shape denote conditionally independent random variables, whereas indices over .event_shape denote dependent random variables (ie one draw from a distribution).\n",
        "- Because the dependent random variables define probability together, the .log_prob() method only produces a single number for each event of shape .event_shape. Thus the total shape of .log_prob() is .batch_shape:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oaj4_D4Q02qk",
        "colab_type": "text"
      },
      "source": [
        "- Note that the Distribution.sample() method also takes a sample_shape parameter that indexes over independent identically distributed (iid) random varables, so that\n",
        "```\n",
        "x2 = d.sample(sample_shape)\n",
        "assert x2.shape == sample_shape + batch_shape + event_shape\n",
        "```\n",
        "- For example univariate distributions have empty event shape (because each number is an independent event). Distributions over vectors like MultivariateNormal have len(event_shape) == 1. Distributions over matrices like InverseWishart have len(event_shape) == 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzCbDWEU0W5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some examples\n",
        "\n",
        "# The simplest distribution shape is a single univariate distribution.\n",
        "d = Bernoulli(0.5)\n",
        "assert d.batch_shape == ()\n",
        "assert d.event_shape == ()\n",
        "\n",
        "x = d.sample()\n",
        "assert x.shape == ()\n",
        "assert d.log_prob(x).shape == ()\n",
        "assert x.shape == d.batch_shape + d.event_shape\n",
        "assert d.log_prob(x).shape == d.batch_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc8gEQCm1BMl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "f8f55f8c-d421-4329-db1a-05cd84df3aa5"
      },
      "source": [
        "print(0.5 * torch.ones(3,4))\n",
        "\n",
        "# Distributions can be batched by passing in batched parameters.\n",
        "d = Bernoulli(0.5 * torch.ones(3,4))\n",
        "assert d.batch_shape == (3, 4) # conditionally independent random variables\n",
        "# event_shape is empty because for any univariate distribution, \n",
        "# samples are independent\n",
        "assert d.event_shape == ()\n",
        "\n",
        "x = d.sample()\n",
        "print(x)\n",
        "assert x.shape == (3, 4)\n",
        "assert d.log_prob(x).shape == (3, 4)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.5000, 0.5000, 0.5000, 0.5000],\n",
            "        [0.5000, 0.5000, 0.5000, 0.5000],\n",
            "        [0.5000, 0.5000, 0.5000, 0.5000]])\n",
            "tensor([[1., 1., 1., 0.],\n",
            "        [0., 1., 0., 1.],\n",
            "        [1., 1., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBjLHsGN1BJt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "4b0390ad-581a-4442-bc6f-ca94a69fe7c7"
      },
      "source": [
        "# Another way to batch distributions is via the .expand() method. \n",
        "# This only works if parameters are identical along the leftmost dimensions.\n",
        "\n",
        "d = Bernoulli(torch.tensor([0.1, 0.2, 0.3, 0.4])).expand([3, 4])\n",
        "print(d)\n",
        "\n",
        "assert d.batch_shape == (3, 4)\n",
        "assert d.event_shape == ()\n",
        "\n",
        "x = d.sample()\n",
        "print(x)\n",
        "assert x.shape == (3, 4)\n",
        "assert d.log_prob(x).shape == (3, 4)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bernoulli(probs: torch.Size([3, 4]))\n",
            "tensor([[0., 1., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me_9dUTy1BGt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ac81c388-862e-4b08-aac2-ee52da18cacd"
      },
      "source": [
        "# Multivariate distributions have nonempty .event_shape. \n",
        "# For these distributions, the shapes of .sample() and .log_prob(x) differ\n",
        "\n",
        "d = MultivariateNormal(torch.zeros(3), torch.eye(3, 3))\n",
        "assert d.batch_shape == () # this generates one sample from the distribution\n",
        "assert d.event_shape == (3,)\n",
        "\n",
        "x = d.sample()\n",
        "print(x)\n",
        "\n",
        "assert x.shape == (3,)            # == batch_shape + event_shape\n",
        "assert d.log_prob(x).shape == ()  # == batch_shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.1487, 1.1159, 0.3977])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhu7pAYp1OjF",
        "colab_type": "text"
      },
      "source": [
        "### Reshaping distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFrC2MJw1BDN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c73ad993-380d-48a3-84d8-d7af8065e6ad"
      },
      "source": [
        "# In Pyro you can treat a univariate distribution as multivariate by calling the .to_event(n) property \n",
        "# where n is the number of batch dimensions (from the right) to declare as dependent.\n",
        "\n",
        "# this would generate 3 samples from a Multivariate Bernoulli distribution (dim=4)\n",
        "d = Bernoulli(0.5 * torch.ones(3,4)).to_event(1)\n",
        "assert d.batch_shape == (3,) \n",
        "assert d.event_shape == (4,)\n",
        "\n",
        "x = d.sample()\n",
        "print(x)\n",
        "\n",
        "assert x.shape == (3, 4)\n",
        "assert d.log_prob(x).shape == (3,) # each sample return a log_prob\n",
        "\n",
        "# samples have shape batch_shape + event_shape, whereas .log_prob(x) values have shape batch_shape. "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 1., 0., 1.],\n",
            "        [1., 1., 0., 1.],\n",
            "        [0., 0., 1., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUGmv1fq0W1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "fc2eca73-f365-440e-e501-fe16a8c66d81"
      },
      "source": [
        "def to_event_test(n):\n",
        "  d = Bernoulli(0.5 * torch.ones(3,4)).to_event(n)\n",
        "  print(d.batch_shape)\n",
        "  print(d.event_shape)\n",
        "\n",
        "  x = d.sample()\n",
        "  print(x)\n",
        "\n",
        "  print(x.shape)\n",
        "  print(d.log_prob(x).shape)\n",
        "  print(\"\\n\")\n",
        "\n",
        "# No more than 2 dimensions\n",
        "to_event_test(0) # A complete conditionally independent sampling\n",
        "to_event_test(1) # Three samples from a Multivariate distribution\n",
        "to_event_test(2) # One sample from 2-dimensional dependent distribution"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.Size([])\n",
            "tensor([[1., 0., 1., 0.],\n",
            "        [0., 1., 1., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "torch.Size([3, 4])\n",
            "torch.Size([3, 4])\n",
            "\n",
            "\n",
            "torch.Size([3])\n",
            "torch.Size([4])\n",
            "tensor([[0., 0., 1., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 1.]])\n",
            "torch.Size([3, 4])\n",
            "torch.Size([3])\n",
            "\n",
            "\n",
            "torch.Size([])\n",
            "torch.Size([3, 4])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "torch.Size([3, 4])\n",
            "torch.Size([])\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLrrJ32f1SyV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "bc8b20a0-072a-4867-e56c-7daaf87056de"
      },
      "source": [
        "# Often in Pyro we’ll declare some dimensions as dependent even though they are in fact independent\n",
        "# Here .expand() would help generate 10 samples - batch_shape = 3\n",
        "\n",
        "x = pyro.sample(\"x\", dist.Normal(0, 1).expand([3]).to_event(1)) # at most 1\n",
        "print(x)\n",
        "assert x.shape == (3, ) # here is a MultiVariate Normal distribution - one sample\n",
        "\n",
        "x = pyro.sample(\"x\", dist.Normal(0, 1).expand([3, 3]).to_event(1)) # at most 2\n",
        "print(x)\n",
        "assert x.shape == (3, 3) # here is a Multivariate Normal distribution - 3 samples"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.1687,  1.0292, -0.7435])\n",
            "tensor([[ 1.3619, -0.9674,  0.7154],\n",
            "        [ 0.2573,  0.3683,  0.9159],\n",
            "        [ 1.1051,  0.6188,  0.7229]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEkYKs6H1hY-",
        "colab_type": "text"
      },
      "source": [
        "### Declaring independent dims with plate\n",
        "\n",
        "- Use the context manager pyro.plate to declare that certain batch dimensions are independent. Inference algorithms can then take advantage of this independence to e.g. construct lower variance gradient estimators or to enumerate in linear space rather than exponential space. \n",
        "- An example of an independent dimension is the index over data in a minibatch: each datum should be independent of all others.\n",
        "- The simplest way to declare a dimension as independent is to declare the rightmost batch dimension as independent via a simple\n",
        "```\n",
        "with pyro.plate(\"my_plate\"):\n",
        "    # within this context, batch dimension -1 is independent\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3-cn3a11i42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plate can make use of conditional independence information when estimating gradients\n",
        "\n",
        "\"\"\"\n",
        "Do NOT run this chunk - just for reference\n",
        "\"\"\"\n",
        "# count from the right by using negative indices like -2, -1\n",
        "# providing an optional size argument to aid in debugging shapes\n",
        "with pyro.plate(\"my_plate\", len(my_data)):\n",
        "    # within this context, batch dimension -1 is independent\n",
        "\n",
        "# nest plates for per-pixel independence\n",
        "with pyro.plate(\"x_axis\", 320):\n",
        "    # within this context, batch dimension -1 is independent\n",
        "    with pyro.plate(\"y_axis\", 200):\n",
        "        # within this context, batch dimensions -2 and -1 are independent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKZxgMAT1sSm",
        "colab_type": "text"
      },
      "source": [
        "- Finally if you want to mix and match plates for e.g. noise that depends only on x, some noise that depends only on y, and some noise that depends on both, you can declare multiple plates and use them as reusable context managers. \n",
        "- In this case Pyro cannot automatically allocate a dimension, so you need to provide a dim argument (again counting from the right):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcU1SfGo1jqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Do NOT run this chunk - just for reference\n",
        "\"\"\"\n",
        "x_axis = pyro.plate(\"x_axis\", 3, dim=-2)\n",
        "y_axis = pyro.plate(\"y_axis\", 2, dim=-3)\n",
        "with x_axis:\n",
        "    # within this context, batch dimension -2 is independent\n",
        "with y_axis:\n",
        "    # within this context, batch dimension -3 is independent\n",
        "with x_axis, y_axis:\n",
        "    # within this context, batch dimensions -3 and -2 are independent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-mS9ZLR1jnh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "17d6f72f-49fc-4389-a363-39c955ce8f55"
      },
      "source": [
        "# Personal view: \n",
        "# for a single number, expand can be skipped/default to match the size \n",
        "with pyro.plate(\"x_plate\", 10):\n",
        "    x = pyro.sample(\"x\", dist.Normal(0, 1))  # .expand([10]) is automatic\n",
        "    print(x)\n",
        "    assert x.shape == (10,)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 2.0762, -0.3551, -1.5438, -0.1028, -1.2283,  0.6111,  1.9926,  1.1711,\n",
            "         0.6425,  0.3730])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-1RoLlD12V9",
        "colab_type": "text"
      },
      "source": [
        "### Take a closer look at batch sizes within plate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNTnqbiY1jlF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "outputId": "0eea58dc-e5eb-474f-8e90-2f50dc8d1dd3"
      },
      "source": [
        "def model1():\n",
        "    a = pyro.sample(\"a\", Normal(0, 1))\n",
        "    print(a)\n",
        "    b = pyro.sample(\"b\", Normal(torch.zeros(2), 1).to_event(1))\n",
        "    print(b)\n",
        "    \n",
        "    # Otherwise, len() == 2 here is to check the shape matching exactly\n",
        "    with pyro.plate(\"c_plate\", 2):\n",
        "        c = pyro.sample(\"c\", Normal(torch.zeros(2), 1))\n",
        "        print(c)\n",
        "    with pyro.plate(\"d_plate\", 3):\n",
        "        # for d here, 2 dimensions from the right would be dependent\n",
        "        d = pyro.sample(\"d\", Normal(torch.zeros(3,4,5), 1).to_event(2))\n",
        "        print(d)\n",
        "        \n",
        "    assert a.shape == ()       # batch_shape == ()     event_shape == ()\n",
        "    assert b.shape == (2,)     # batch_shape == ()     event_shape == (2,)\n",
        "    assert c.shape == (2,)     # batch_shape == (2,)   event_shape == ()\n",
        "    assert d.shape == (3,4,5)  # batch_shape == (3,)   event_shape == (4,5)\n",
        "\n",
        "    x_axis = pyro.plate(\"x_axis\", 3, dim=-2)\n",
        "    y_axis = pyro.plate(\"y_axis\", 2, dim=-3)\n",
        "    # dim (int) – An optional dimension to use for this independence index. \n",
        "    # If specified, dim should be negative, i.e. should index from the right. \n",
        "    # If not specified, dim is set to the rightmost dim that is left of all enclosing plate contexts.\n",
        "    \n",
        "    with x_axis:\n",
        "        x = pyro.sample(\"x\", Normal(0, 1))\n",
        "        print(x)\n",
        "    with y_axis:\n",
        "        y = pyro.sample(\"y\", Normal(0, 1))\n",
        "    with x_axis, y_axis:\n",
        "        xy = pyro.sample(\"xy\", Normal(0, 1))\n",
        "        z = pyro.sample(\"z\", Normal(0, 1).expand([5]).to_event(1))\n",
        "        \n",
        "    assert x.shape == (3, 1)        # batch_shape == (3,1)     event_shape == ()\n",
        "    assert y.shape == (2, 1, 1)     # batch_shape == (2,1,1)   event_shape == ()\n",
        "    assert xy.shape == (2, 3, 1)    # batch_shape == (2,3,1)   event_shape == ()\n",
        "    assert z.shape == (2, 3, 1, 5)  # batch_shape == (2,3,1)   event_shape == (5,)\n",
        "\n",
        "test_model(model1, model1, Trace_ELBO())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-1.1764)\n",
            "tensor([0.2359, 1.1323])\n",
            "tensor([ 0.2678, -1.5047])\n",
            "tensor([[[-1.1603,  0.0037,  0.5752, -1.8553, -0.2216],\n",
            "         [-1.2801,  1.0434, -0.2138,  0.9126,  0.3099],\n",
            "         [-1.1750,  0.9498,  1.7685,  0.9414, -0.9941],\n",
            "         [ 0.2179,  0.1842,  1.9193, -0.5476, -0.6444]],\n",
            "\n",
            "        [[ 1.4856, -0.0487, -0.6052, -0.0714, -0.6591],\n",
            "         [-0.5913,  0.0549,  0.5166, -1.7880, -0.9646],\n",
            "         [ 2.3067,  0.7811,  0.1099, -2.4119,  0.8760],\n",
            "         [ 1.0870, -0.5308, -2.2917,  0.3445, -0.3632]],\n",
            "\n",
            "        [[ 1.3317, -1.5827,  0.1967,  1.2679,  1.8436],\n",
            "         [ 2.4306, -1.0752,  0.2661, -1.5314, -0.2388],\n",
            "         [-1.5428, -0.7235,  0.2389,  0.3362, -1.9714],\n",
            "         [-0.9448, -0.1025, -0.3308, -1.3757, -0.3435]]])\n",
            "tensor([[-0.9617],\n",
            "        [-1.2781],\n",
            "        [-0.0134]])\n",
            "tensor(-1.1764)\n",
            "tensor([0.2359, 1.1323])\n",
            "tensor([ 0.2678, -1.5047])\n",
            "tensor([[[-1.1603,  0.0037,  0.5752, -1.8553, -0.2216],\n",
            "         [-1.2801,  1.0434, -0.2138,  0.9126,  0.3099],\n",
            "         [-1.1750,  0.9498,  1.7685,  0.9414, -0.9941],\n",
            "         [ 0.2179,  0.1842,  1.9193, -0.5476, -0.6444]],\n",
            "\n",
            "        [[ 1.4856, -0.0487, -0.6052, -0.0714, -0.6591],\n",
            "         [-0.5913,  0.0549,  0.5166, -1.7880, -0.9646],\n",
            "         [ 2.3067,  0.7811,  0.1099, -2.4119,  0.8760],\n",
            "         [ 1.0870, -0.5308, -2.2917,  0.3445, -0.3632]],\n",
            "\n",
            "        [[ 1.3317, -1.5827,  0.1967,  1.2679,  1.8436],\n",
            "         [ 2.4306, -1.0752,  0.2661, -1.5314, -0.2388],\n",
            "         [-1.5428, -0.7235,  0.2389,  0.3362, -1.9714],\n",
            "         [-0.9448, -0.1025, -0.3308, -1.3757, -0.3435]]])\n",
            "tensor([[-0.9617],\n",
            "        [-1.2781],\n",
            "        [-0.0134]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKT6HP8Z16IV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82095d30-dcbf-4e07-83d9-40eef2ad67de"
      },
      "source": [
        "trace = poutine.trace(model1).get_trace()\n",
        "trace.compute_log_prob()  # optional, but allows printing of log_prob shapes\n",
        "print(trace.format_shapes())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-1.5571)\n",
            "tensor([ 2.1526, -0.2416])\n",
            "tensor([-0.5817, -0.2862])\n",
            "tensor([[[ 1.7491,  1.2164, -1.5942, -0.9981, -0.6127],\n",
            "         [-1.0252,  1.6232,  1.0279,  0.1608,  0.1901],\n",
            "         [ 0.3066,  0.8032,  0.0299, -0.4769,  0.4482],\n",
            "         [-0.1270,  0.1544, -0.9237, -0.8893,  1.2569]],\n",
            "\n",
            "        [[ 1.3002,  1.2701, -1.4480,  0.9792,  0.2713],\n",
            "         [-0.2358, -0.2328, -0.3218,  0.6783, -0.0649],\n",
            "         [ 1.3138, -1.1190,  0.4195, -0.5550, -0.5623],\n",
            "         [-0.1518,  0.9876, -1.1545, -0.0756,  0.1612]],\n",
            "\n",
            "        [[-0.2436,  0.5915, -0.7203,  1.2710, -0.5661],\n",
            "         [-2.2154, -0.2767,  1.5700, -0.5976,  0.3555],\n",
            "         [-0.5456, -2.2799,  0.8896, -1.5686, -0.0903],\n",
            "         [ 0.7392,  1.5704, -0.2646,  0.2411, -0.1154]]])\n",
            "tensor([[-0.8905],\n",
            "        [-1.5685],\n",
            "        [-0.8145]])\n",
            "Trace Shapes:            \n",
            " Param Sites:            \n",
            "Sample Sites:            \n",
            "       a dist       |    \n",
            "        value       |    \n",
            "     log_prob       |    \n",
            "       b dist       | 2  \n",
            "        value       | 2  \n",
            "     log_prob       |    \n",
            " c_plate dist       |    \n",
            "        value     2 |    \n",
            "     log_prob       |    \n",
            "       c dist     2 |    \n",
            "        value     2 |    \n",
            "     log_prob     2 |    \n",
            " d_plate dist       |    \n",
            "        value     3 |    \n",
            "     log_prob       |    \n",
            "       d dist     3 | 4 5\n",
            "        value     3 | 4 5\n",
            "     log_prob     3 |    \n",
            "  x_axis dist       |    \n",
            "        value     3 |    \n",
            "     log_prob       |    \n",
            "  y_axis dist       |    \n",
            "        value     2 |    \n",
            "     log_prob       |    \n",
            "       x dist   3 1 |    \n",
            "        value   3 1 |    \n",
            "     log_prob   3 1 |    \n",
            "       y dist 2 1 1 |    \n",
            "        value 2 1 1 |    \n",
            "     log_prob 2 1 1 |    \n",
            "      xy dist 2 3 1 |    \n",
            "        value 2 3 1 |    \n",
            "     log_prob 2 3 1 |    \n",
            "       z dist 2 3 1 | 5  \n",
            "        value 2 3 1 | 5  \n",
            "     log_prob 2 3 1 |    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm667bEw2BH2",
        "colab_type": "text"
      },
      "source": [
        "### Subsampling tensors inside a plate\n",
        "\n",
        "- To subsample data, you need to inform Pyro of both the original data size and the subsample size; Pyro will then choose a random subset of data and yield the set of indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edkCKzIo2CDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "142d79fa-30a5-4b69-e986-5574386499b4"
      },
      "source": [
        "data = torch.arange(100.)\n",
        "print(data)\n",
        "\n",
        "def model2():\n",
        "    mean = pyro.param(\"mean\", torch.zeros(len(data)))\n",
        "    with pyro.plate(\"data\", len(data), subsample_size=10) as ind:\n",
        "        assert len(ind) == 10    # ind is a LongTensor that indexes the subsample.\n",
        "        batch = data[ind]        # Select a minibatch of data.\n",
        "        mean_batch = mean[ind]   # Take care to select the relevant per-datum parameters.\n",
        "        # Do stuff with batch:\n",
        "        x = pyro.sample(\"x\", Normal(mean_batch, 1), obs=batch)\n",
        "        assert len(x) == 10\n",
        "\n",
        "test_model(model2, guide=lambda: None, loss=Trace_ELBO())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
            "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
            "        28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41.,\n",
            "        42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55.,\n",
            "        56., 57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69.,\n",
            "        70., 71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83.,\n",
            "        84., 85., 86., 87., 88., 89., 90., 91., 92., 93., 94., 95., 96., 97.,\n",
            "        98., 99.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bof3nxU2JGe",
        "colab_type": "text"
      },
      "source": [
        "### Broadcasting to allow parallel enumeration\n",
        "\n",
        "- To use parallel enumeration, Pyro needs to allocate tensor dimension that it can use for enumeration. To avoid conflicting with other dimensions that we want to use for plates, we need to declare a budget of the maximum number of tensor dimensions we’ll use. \n",
        "\n",
        "- This budget is called max_plate_nesting and is an argument to SVI (the argument is simply passed through to TraceEnum_ELBO). Usually Pyro can determine this budget on its own (it runs the (model,guide) pair once and record what happens), but in case of dynamic model structure you may need to declare max_plate_nesting manually.\n",
        "\n",
        "To understand max_plate_nesting and how Pyro allocates dimensions for enumeration, let’s revisit model1() from above. This time we’ll map out three types of dimensions: enumeration dimensions on the left (Pyro takes control of these), batch dimensions in the middle, and event dimensions on the right."
      ]
    }
  ]
}