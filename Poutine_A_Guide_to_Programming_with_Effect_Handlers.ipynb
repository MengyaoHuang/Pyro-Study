{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Poutine: A Guide to Programming with Effect Handlers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0OpAwphdqF4",
        "colab_type": "text"
      },
      "source": [
        "## Poutine: A Guide to Programming with Effect Handlers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxGQKSZ3d-ex",
        "colab_type": "code",
        "outputId": "d642f494-e889-412e-9b29-db6cd9e08731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "source": [
        "!pip install pyro-ppl"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyro-ppl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/0e/0523cb040c8f3ee8644b4280f6a72ed598ac7864680b667d6052fb5d445a/pyro-ppl-0.3.4.tar.gz (262kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.5.5)\n",
            "Requirement already satisfied: graphviz>=0.8 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.16.4)\n",
            "Collecting opt_einsum>=2.3.2 (from pyro-ppl)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 20.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.12.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.1.0)\n",
            "Collecting tqdm>=4.31 (from pyro-ppl)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/3d/7a6b68b631d2ab54975f3a4863f3c4e9b26445353264ef01f465dc9b0208/tqdm-4.32.2-py2.py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 15.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyro-ppl, opt-einsum\n",
            "  Building wheel for pyro-ppl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/de/b5/88300d2adc973a7ec963b339d2935d34a0cf02c08b613a8a5e\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
            "Successfully built pyro-ppl opt-einsum\n",
            "Installing collected packages: opt-einsum, tqdm, pyro-ppl\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed opt-einsum-2.3.2 pyro-ppl-0.3.4 tqdm-4.32.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnSBJFZydsFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "import pyro.poutine as poutine\n",
        "\n",
        "from pyro.poutine.runtime import effectful\n",
        "\n",
        "pyro.set_rng_seed(101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6bBZcxpl6RN",
        "colab_type": "text"
      },
      "source": [
        "### A first example for joint probability distribution inference\n",
        "\n",
        "- This model below defines a joint probability distribution over \"weight\" and \"measurement\":\n",
        "  - weight|guess ~ Normal(guess, 1)\n",
        "  - measurement|guess|weight ~ Normal(weight, 0.75)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55JMpgPOeEjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scale(guess):\n",
        "    weight = pyro.sample(\"weight\", dist.Normal(guess, 1.0))\n",
        "    return pyro.sample(\"measurement\", dist.Normal(weight, 0.75))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqxybkUhnJu1",
        "colab_type": "text"
      },
      "source": [
        "- If we had access to the inputs and outputs of each pyro.sample site, we could compute their log-joint:\n",
        "```\n",
        "# This is formatted as code\n",
        "logp = dist.Normal(guess, 1.0).log_prob(weight).sum() + dist.Normal(weight, 0.75).log_prob(measurement).sum()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm6cuKJrn4yt",
        "colab_type": "text"
      },
      "source": [
        "### A first look at Poutine: Pyro’s library of algorithmic building blocks\n",
        "\n",
        "- Poutine is an Effect handlers library provided in Pyro\n",
        "- Compose two existing effect handlers first:\n",
        "  - poutine.condition: sets output values of pyro.sample statements\n",
        "  - poutine.trace: records the inputs, distributions, and outputs of pyro.sample statements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saY1C7SmsP6b",
        "colab_type": "text"
      },
      "source": [
        "- conditionMessenger class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WYilsP2sTaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Do NOT run this chunk - just for reference\n",
        "\"\"\"\n",
        "\n",
        "# Adds values at observe sites to condition on data and override sampling\n",
        "class ConditionMessenger(Messenger):\n",
        "    def __init__(self, data):\n",
        "        # data here would be a dictionary or a Trace\n",
        "        super(ConditionMessenger, self).__init__()\n",
        "        self.data = data\n",
        "\n",
        "    def _pyro_sample(self, msg):\n",
        "        # msg here would be current message at a trace site\n",
        "        # returns a sample from the stochastic function at the site\n",
        "\n",
        "        # If msg[\"name\"] appears in self.data, \n",
        "        # convert the sample site into an observe site\n",
        "        # whose observed value is the value from self.data[msg[\"name\"]].\n",
        "        # Otherwise, implements default sampling behavior\n",
        "        # with no additional effects.\n",
        "\n",
        "        name = msg[\"name\"]\n",
        "        if name in self.data:\n",
        "            assert not msg[\"is_observed\"], \\\n",
        "                \"should not change values of existing observes\"\n",
        "            if isinstance(self.data, Trace):\n",
        "                msg[\"value\"] = self.data.nodes[name][\"value\"]\n",
        "            else:\n",
        "                msg[\"value\"] = self.data[name]\n",
        "            msg[\"is_observed\"] = True\n",
        "        return None\n",
        "      \n",
        "    def _pyro_param(self, msg):\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QPS1bc9eEdY",
        "colab_type": "code",
        "outputId": "60c85a03-154a-45b6-cfd2-1f281401563d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def make_log_joint(model):\n",
        "    def _log_joint(cond_data, *args, **kwargs):\n",
        "        conditioned_model = poutine.condition(model, data=cond_data)\n",
        "        trace = poutine.trace(conditioned_model).get_trace(*args, **kwargs)\n",
        "        return trace.log_prob_sum()\n",
        "    return _log_joint\n",
        "\n",
        "scale_log_joint = make_log_joint(scale)\n",
        "print(scale_log_joint({\"measurement\": 9.5, \"weight\": 8.23}, 8.5))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-3.0203)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecObSX4_qs_-",
        "colab_type": "code",
        "outputId": "d8087b53-ba5a-449a-d250-7cddf09116f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Further explanations for chunk above\n",
        "\n",
        "# Poutine.trace and poutine.condition are wrappers for context managers \n",
        "# that presumably communicate with the model through something inside pyro.sample\n",
        "\n",
        "# Poutine.trace produces a data structure (a Trace) containing a dictionary \n",
        "# whose keys are sample site names and values are dictionaries \n",
        "# containing the distribution (\"fn\") and output (\"value\") at each site\n",
        "\n",
        "# Again to reminder: \n",
        "# A Messenger is placed at the bottom of the stack when its enter method is called, \n",
        "# i.e. when it is used in a “with” statement\n",
        "\n",
        "\n",
        "from pyro.poutine.trace_messenger import TraceMessenger\n",
        "from pyro.poutine.condition_messenger import ConditionMessenger\n",
        "\n",
        "def make_log_joint_2(model):\n",
        "    def _log_joint(cond_data, *args, **kwargs):\n",
        "        with TraceMessenger() as tracer:\n",
        "            with ConditionMessenger(data=cond_data):\n",
        "                # here sample \"weight\" and \"measurement\" in model \"scale\"\n",
        "                model(*args, **kwargs)\n",
        "\n",
        "        trace = tracer.trace\n",
        "        logp = 0.\n",
        "        # here trace records all msg regardless of type\n",
        "        for name, node in trace.nodes.items():\n",
        "            if node[\"type\"] == \"sample\":\n",
        "                if node[\"is_observed\"]:\n",
        "                    assert node[\"value\"] is cond_data[name]\n",
        "                logp = logp + node[\"fn\"].log_prob(node[\"value\"]).sum()\n",
        "        return logp\n",
        "    return _log_joint\n",
        "\n",
        "scale_log_joint = make_log_joint_2(scale)\n",
        "\n",
        "# here 8.5 below is the input for guess in model \"scale\"\n",
        "# dictionary is the input for conditionMessenger\n",
        "\n",
        "# explanation: if our cond_data provides all values then use given values\n",
        "# to calculate the log_probs. Otherwise, we would follow the sampled values before - conditionMessenger operates\n",
        "print(scale_log_joint({}, 8.5))\n",
        "print(scale_log_joint({\"measurement\": 9.5}, 8.5))\n",
        "scale_log_joint({\"measurement\": 9.5, \"weight\": 8.23}, 8.5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-2.8493)\n",
            "tensor(-3.1514)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-3.0203)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nifipXx0MmM",
        "colab_type": "text"
      },
      "source": [
        "### Implementing new effect handlers with the Messenger API - a more complicated user-defined Messenger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9lXn-m-8gdq",
        "colab_type": "code",
        "outputId": "f6cff866-9572-499b-e28b-f033f71ef3db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# __init__ and __call__ in python\n",
        "class A:\n",
        "  def __init__(self):\n",
        "    print(\"init\")\n",
        "  def __call__(self):\n",
        "    print(\"call\")\n",
        "    \n",
        "# happen during initialization\n",
        "a = A()\n",
        "# happen when the class is called\n",
        "a()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init\n",
            "call\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rh0qqil_5XZ",
        "colab_type": "text"
      },
      "source": [
        "### A tip for using clone() below in the LogJointMessenger\n",
        "\n",
        "- tensor.detach() creates a tensor that shares storage with tensor that does not require grad. \n",
        "- tensor.clone()creates a copy of tensor that imitates the original tensor's requires_grad field.\n",
        "- use detach() when attempting to remove a tensor from a computation graph, and clone as a way to copy the tensor while still keeping the copy as a part of the computation graph it came from.\n",
        "- tensor.data returns a new tensor that shares storage with tensor. However, it always has requires_grad=False (even if the original tensor had requires_grad=True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaB7U_Fpqs77",
        "colab_type": "code",
        "outputId": "7ffb43b8-8161-4d2f-9661-b0076d148478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "class LogJointMessenger(poutine.messenger.Messenger):\n",
        "\n",
        "    def __init__(self, cond_data):\n",
        "        self.data = cond_data\n",
        "\n",
        "    # __call__ is syntactic sugar for using Messengers as higher-order functions.\n",
        "    # Messenger already defines __call__, but we re-define it here\n",
        "    # for exposition and to change the return value:\n",
        "    def __call__(self, fn):\n",
        "        def _fn(*args, **kwargs):\n",
        "            # this with command would call the messenger itself\n",
        "            # to handle messages from running \"fn\"\n",
        "            with self:\n",
        "                fn(*args, **kwargs)\n",
        "                # return self.logp\n",
        "                return self.logp.clone()\n",
        "        return _fn\n",
        "    \n",
        "    # Always override __enter__ and __exit__ when using new Messenger!\n",
        "    \n",
        "    def __enter__(self):\n",
        "        self.logp = torch.tensor(0.)\n",
        "        # All Messenger subclasses must call the base Messenger.__enter__()\n",
        "        # in their __enter__ methods\n",
        "        # __enter__ would push Messenger itself to the bottom of the stack\n",
        "        return super(LogJointMessenger, self).__enter__()\n",
        "\n",
        "    # __exit__ takes the same arguments in all Python context managers\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.logp = torch.tensor(0.)\n",
        "        # All Messenger subclasses must call the base Messenger.__exit__ method\n",
        "        # in their __exit__ methods.\n",
        "        return super(LogJointMessenger, self).__exit__(exc_type, exc_value, traceback)\n",
        "\n",
        "    # _pyro_sample will be called once per pyro.sample site.\n",
        "    # It takes a dictionary msg containing the name, distribution,\n",
        "    # observation or sample value, and other metadata from the sample site.\n",
        "    # work as __process__messsage__ in class Messenger\n",
        "    def _pyro_sample(self, msg):\n",
        "        assert msg[\"name\"] in self.data\n",
        "        msg[\"value\"] = self.data[msg[\"name\"]]\n",
        "        # Since we've observed a value for this site, we set the \"is_observed\" flag to True\n",
        "        # This tells any other Messengers not to overwrite msg[\"value\"] with a sample.\n",
        "        msg[\"is_observed\"] = True\n",
        "        # \"scale\": torch.tensor(1.) - a key in msg dic\n",
        "        # Multiplicative scale factor that can be applied to each site's log_prob\n",
        "        self.logp = self.logp + (msg[\"scale\"] * msg[\"fn\"].log_prob(msg[\"value\"])).sum()\n",
        "\n",
        "        \n",
        "# add the LogJointMessenger into the handler stack to process all messages generated during model \"scale\"\n",
        "with LogJointMessenger(cond_data={\"measurement\": 9.5, \"weight\": 8.23}) as m:\n",
        "    scale(8.5)\n",
        "    # print(m.logp)\n",
        "    print(m.logp.clone())\n",
        "\n",
        "scale_log_joint = LogJointMessenger(cond_data={\"measurement\": 9.5, \"weight\": 8.23})(scale)\n",
        "print(scale_log_joint(8.5))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-3.0203)\n",
            "tensor(-3.0203)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9PdPgYIqs5D",
        "colab_type": "code",
        "outputId": "c9abfbab-0b5f-40e6-ebea-0a774093b8fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# A common way to use LogJointMessenger as a context wrapper with in a function\n",
        "def log_joint(model=None, cond_data=None):\n",
        "    msngr = LogJointMessenger(cond_data=cond_data)\n",
        "    return msngr(model) if model is not None else msngr\n",
        "\n",
        "# All msgs generated in model \"scale\" must be in keys provided in cond_data\n",
        "scale_log_joint = log_joint(scale, cond_data={\"measurement\": 9.5, \"weight\": 8.23})\n",
        "print(scale_log_joint(8.5))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-3.0203)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSmGX_QTAtZh",
        "colab_type": "text"
      },
      "source": [
        "### Extension to the LogJointMessenger example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gNza2ZpA7Ax",
        "colab_type": "code",
        "outputId": "fb403317-a0ce-4652-df40-da6b54576a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "class LogJointMessenger2(poutine.messenger.Messenger):\n",
        "  \n",
        "    def __init__(self, cond_data):\n",
        "        self.data = cond_data\n",
        "    def __call__(self, fn):\n",
        "        def _fn(*args, **kwargs):\n",
        "            with self:\n",
        "                fn(*args, **kwargs)\n",
        "                return self.logp.clone()\n",
        "        return _fn\n",
        "    def __enter__(self):\n",
        "        self.logp = torch.tensor(0.)\n",
        "        return super(LogJointMessenger2, self).__enter__()\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.logp = torch.tensor(0.)\n",
        "        return super(LogJointMessenger2, self).__exit__(exc_type, exc_value, traceback)  \n",
        "      \n",
        "    def _pyro_sample(self, msg):\n",
        "        if msg[\"name\"] in self.data:\n",
        "            msg[\"value\"] = self.data[msg[\"name\"]]\n",
        "            msg[\"done\"] = True\n",
        "    \n",
        "    # necessary because some effects can only be applied \n",
        "    # after all other effect handlers have had a chance to update the message once\n",
        "    def _pyro_post_sample(self, msg):\n",
        "        assert msg[\"done\"]  # the \"done\" flag asserts that no more modifications to value and fn will be performed.\n",
        "        print(msg[\"name\"])\n",
        "        self.logp = self.logp + (msg[\"scale\"] * msg[\"fn\"].log_prob(msg[\"value\"])).sum()\n",
        "        \n",
        "with LogJointMessenger2(cond_data={\"measurement\": 9.5}) as m:\n",
        "    # Here weight is not the cond_data dic but its msg[\"done\"] is True\n",
        "    # after being operated by other Messengers or default when stack is empty?\n",
        "    scale(8.5)\n",
        "    print(m.logp)\n",
        "\n",
        "with LogJointMessenger2(cond_data={\"measurement\": 9.5, \"weight\": 8.23}) as m:\n",
        "    scale(8.5)\n",
        "    print(m.logp)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weight\n",
            "measurement\n",
            "tensor(-1.8835)\n",
            "weight\n",
            "measurement\n",
            "tensor(-3.0203)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycgefNZMGUvI",
        "colab_type": "text"
      },
      "source": [
        "### Inside the messages sent by Messengers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiN4AXOaAdXZ",
        "colab_type": "code",
        "outputId": "f44b9753-044c-4b55-e1ea-11ff0e1d9ff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "msg = {\n",
        "    # The following fields contain the name, inputs, function, and output of a site.\n",
        "    # These are generally the only fields you'll need to think about.\n",
        "    \"name\": \"x\",\n",
        "    \"fn\": dist.Bernoulli(0.5),\n",
        "    \"value\": None,  # msg[\"value\"] will eventually contain the value returned by pyro.sample\n",
        "    \"is_observed\": False,  # because obs=None by default; only used by sample sites\n",
        "    \n",
        "    \"args\": (),  # positional arguments passed to \"fn\" when it is called; usually empty for sample sites\n",
        "    \"kwargs\": {},  # keyword arguments passed to \"fn\" when it is called; usually empty for sample sites\n",
        "    \n",
        "    # This field typically contains metadata needed or stored by a particular inference algorithm\n",
        "    \"infer\": {\"enumerate\": \"parallel\"},\n",
        "    \n",
        "    # The remaining fields are generally only used by Pyro's internals,\n",
        "    # or for implementing more advanced effects beyond the scope of this tutorial\n",
        "    \"type\": \"sample\",  # label used by Messenger._process_message to dispatch, in this case to _pyro_sample\n",
        "    \"done\": False,\n",
        "    \"stop\": False,\n",
        "    \"scale\": torch.tensor(1.),  # Multiplicative scale factor that can be applied to each site's log_prob\n",
        "    \"mask\": None,\n",
        "    \"continuation\": None,\n",
        "    \"cond_indep_stack\": (),  # Will contain metadata from each pyro.plate enclosing this sample site.\n",
        "}\n",
        "pyro.sample(\"x\", dist.Bernoulli(0.5), infer={\"enumerate\": \"parallel\"}, obs=None)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gnmd79fUaqS",
        "colab_type": "text"
      },
      "source": [
        "### Implementing inference algorithms with existing effect handlers: examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT-mZVOpUl05",
        "colab_type": "text"
      },
      "source": [
        "### Example1: Variational inference with a Monte Carlo ELBO\n",
        "\n",
        "- ELBO training attached in the Mini Pyro page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntq93LrPAdU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def monte_carlo_elbo(model, guide, batch, *args, **kwargs):\n",
        "    # assuming batch is a dictionary, we use poutine.condition to fix values of observed variables\n",
        "    conditioned_model = poutine.condition(model, data=batch)\n",
        "\n",
        "    # we'll approximate the expectation in the ELBO with a single sample:\n",
        "    # first, we run the guide forward unmodified and record values and distributions\n",
        "    # at each sample site using poutine.trace\n",
        "    guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n",
        "\n",
        "    # we use poutine.replay to set the values of latent variables in the model\n",
        "    # to the values sampled above by our guide, and use poutine.trace\n",
        "    # to record the distributions that appear at each sample site in in the model\n",
        "    model_trace = poutine.trace(poutine.replay(conditioned_model, \n",
        "                                               trace=guide_trace)).get_trace(*args, **kwargs)\n",
        "    \n",
        "    elbo = 0.\n",
        "    for name, node in model_trace.nodes.items():\n",
        "        if node[\"type\"] == \"sample\":\n",
        "            elbo = elbo + node[\"fn\"].log_prob(node[\"value\"]).sum()\n",
        "            if not node[\"is_observed\"]:\n",
        "                elbo = elbo - guide_trace.nodes[name][\"fn\"].log_prob(node[\"value\"]).sum()\n",
        "    return -elbo\n",
        "\n",
        "  \n",
        "# use poutine.trace and poutine.block to record pyro.param calls for optimization\n",
        "def train(model, guide, data):\n",
        "    optimizer = pyro.optim.Adam({})\n",
        "    for batch in data:\n",
        "        # this poutine.trace will record all of the parameters that appear in the model and guide\n",
        "        # during the execution of monte_carlo_elbo\n",
        "        with poutine.trace() as param_capture:\n",
        "            # we use poutine.block here so that only parameters appear in the trace above\n",
        "            with poutine.block(hide_fn=lambda node: node[\"type\"] != \"param\"):\n",
        "                loss = monte_carlo_elbo(model, guide, batch)\n",
        "\n",
        "        loss.backward()\n",
        "        params = set(node[\"value\"].unconstrained()\n",
        "                     for node in param_capture.trace.nodes.values())\n",
        "        optimizer.step(params)\n",
        "        pyro.infer.util.zero_grads(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTpe1g3qVxxh",
        "colab_type": "text"
      },
      "source": [
        "### Example2: Exact inference via sequential enumeration\n",
        "\n",
        "- This example uses poutine.queue, itself implemented using poutine.trace, poutine.replay, and poutine.block, to enumerate over possible values of all discrete variables in a model and compute a marginal distribution over all possible return values or the possible values at a particular sample site."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Rh18wJImXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Messenger that does a nonlocal exit by raising a util.NonlocalExit exception\n",
        "class EscapeMessenger(Messenger):\n",
        "  \n",
        "    def __init__(self, escape_fn):\n",
        "        # escape_fn: function that takes a msg as input and returns True\n",
        "        # if the poutine should perform a nonlocal exit at that site.\n",
        "        super(EscapeMessenger, self).__init__()\n",
        "        self.escape_fn = escape_fn\n",
        "\n",
        "    def _pyro_sample(self, msg):\n",
        "        # returns a sample from the stochastic function at the site.\n",
        "        # Evaluates self.escape_fn on the site (self.escape_fn(msg)).\n",
        "        # If this returns True, raises an exception NonlocalExit(msg).\n",
        "        # Else, implements default _pyro_sample behavior with no additional effects.\n",
        "        if self.escape_fn(msg):\n",
        "            msg[\"done\"] = True\n",
        "            msg[\"stop\"] = True\n",
        "\n",
        "            def cont(m):\n",
        "                raise NonlocalExit(m)\n",
        "            msg[\"continuation\"] = cont\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NemvYbnEeXt2",
        "colab_type": "text"
      },
      "source": [
        "# Remain to Check!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS57siN3E9vU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8cb166a3-7268-45e7-f718-0b558b12d38d"
      },
      "source": [
        "# Some preparation\n",
        "\n",
        "# Initialize queue\n",
        "temp = queue.Queue(10)\n",
        "# Insert Element\n",
        "temp.put(2)\n",
        "# Get And remove the element\n",
        "temp.get()\n",
        "\n",
        "# functool.partial\n",
        "# keep part of args and keywords and extend any updates\n",
        "def partial(func, *args, **keywords):\n",
        "    def newfunc(*fargs, **fkeywords):\n",
        "        newkeywords = keywords.copy()\n",
        "        newkeywords.update(fkeywords)\n",
        "        return func(*(args + fargs), **newkeywords)\n",
        "    newfunc.func = func\n",
        "    newfunc.args = args\n",
        "    newfunc.keywords = keywords\n",
        "    return newfunc\n",
        "\n",
        "# poutine.queue function for sequential enumeration over discrete variables\n",
        "# Given a stochastic function and a queue,\n",
        "# return a return value from a complete trace in the queue.\n",
        "def queue(fn=None, queue=None, max_tries=None,\n",
        "          extend_fn=None, escape_fn=None, num_samples=None):\n",
        "    \"\"\"\n",
        "    :param fn: a stochastic function (callable containing Pyro primitive calls)\n",
        "    :param queue: a queue data structure like multiprocessing.Queue to hold partial traces\n",
        "    :param max_tries: maximum number of attempts to compute a single complete trace\n",
        "    :param extend_fn: function (possibly stochastic) that takes a partial trace and a site,\n",
        "        and returns a list of extended traces\n",
        "    :param escape_fn: function (possibly stochastic) that takes a partial trace and a site,\n",
        "        and returns a boolean value to decide whether to exit\n",
        "    :param num_samples: optional number of extended traces for extend_fn to return\n",
        "    :returns: stochastic function decorated with poutine logic\n",
        "    \"\"\"\n",
        "    if max_tries is None:\n",
        "        max_tries = int(1e6)\n",
        "    if extend_fn is None:\n",
        "        extend_fn = util.enum_extend\n",
        "    if escape_fn is None:\n",
        "        escape_fn = util.discrete_escape\n",
        "    if num_samples is None:\n",
        "        num_samples = -1\n",
        "\n",
        "    def wrapper(wrapped):\n",
        "        def _fn(*args, **kwargs):\n",
        "            for i in range(max_tries):\n",
        "                assert not queue.empty(), \\\n",
        "                    \"trying to get() from an empty queue will deadlock\"\n",
        "                next_trace = queue.get()\n",
        "                try:\n",
        "                    ftr = trace(escape(replay(wrapped, trace=next_trace), escape_fn=functools.partial(escape_fn,next_trace)))\n",
        "                    return ftr(*args, **kwargs)\n",
        "                except NonlocalExit as site_container:\n",
        "                    site_container.reset_stack()\n",
        "                    for tr in extend_fn(ftr.trace.copy(),site_container.site,num_samples=num_samples):\n",
        "                        queue.put(tr)\n",
        "                        \n",
        "            raise ValueError(\"max tries ({}) exceeded\".format(str(max_tries)))\n",
        "        return _fn\n",
        "    return wrapper(fn) if fn is not None else wrapper"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0TEnrhkU1Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sequential_discrete_marginal(model, data, site_name=\"_RETURN\"):\n",
        "\n",
        "    from six.moves import queue  # queue data structures\n",
        "    q = queue.Queue()  # Instantiate a first-in first-out queue\n",
        "    q.put(poutine.Trace())  # seed the queue with an empty trace\n",
        "\n",
        "    # as before, we fix the values of observed random variables with poutine.condition\n",
        "    # assuming data is a dictionary whose keys are names of sample sites in model\n",
        "    conditioned_model = poutine.condition(model, data=data)\n",
        "\n",
        "    # we wrap the conditioned model in a poutine.queue,\n",
        "    # which repeatedly pushes and pops partially completed executions from a Queue()\n",
        "    # to perform breadth-first enumeration over the set of values of all discrete sample sites in model\n",
        "    enum_model = poutine.queue(conditioned_model, queue=q)\n",
        "\n",
        "    # actually perform the enumeration by repeatedly tracing enum_model\n",
        "    # and accumulate samples and trace log-probabilities for postprocessing\n",
        "    samples, log_weights = [], []\n",
        "    while not q.empty():\n",
        "        trace = poutine.trace(enum_model).get_trace()\n",
        "        samples.append(trace.nodes[site_name][\"value\"])\n",
        "        log_weights.append(trace.log_prob_sum())\n",
        "\n",
        "    # we take the samples and log-joints and turn them into a histogram:\n",
        "    samples = torch.stack(samples, 0)\n",
        "    log_weights = torch.stack(log_weights, 0)\n",
        "    log_weights = log_weights - dist.util.logsumexp(log_weights, dim=0)\n",
        "    # Empirical distribution associated with the sampled data\n",
        "    return dist.Empirical(samples, log_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZygNWF_Dorx",
        "colab_type": "text"
      },
      "source": [
        "### Example3: implementing lazy evaluation with the Messenger API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLw6RtICUdAd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b5eaeff6-b22a-42b7-8793-59ccc21e46f4"
      },
      "source": [
        "class Foo:\n",
        "  a = 5\n",
        "fooInstance = Foo()\n",
        "print(isinstance(fooInstance, Foo))\n",
        "print(isinstance(fooInstance, (list, tuple)))\n",
        "print(isinstance(fooInstance, (list, tuple, Foo)))\n",
        "\n",
        "# isinstance(object, classinfo)\n",
        "# object - object to be checked\n",
        "# classinfo - class, type, or tuple of classes and types\n",
        "# True if the object is an instance or subclass of a class, or any element of the tuple\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG_UNkNMU1GZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first define a LazyValue class that we will use to build up a computation graph\n",
        "\n",
        "# With LazyValue, implementing lazy evaluation as a Messenger compatible with other effect handlers is suprisingly easy. \n",
        "# We just make each msg[\"value\"] a LazyValue and introduce a new operation type \"apply\" for deterministic operations\n",
        "class LazyValue(object):\n",
        "    def __init__(self, fn, *args, **kwargs):\n",
        "        self._expr = (fn, args, kwargs)\n",
        "        self._value = None\n",
        "        \n",
        "    def __str__(self):\n",
        "        return \"({} {})\".format(str(self._expr[0]), \" \".join(map(str, self._expr[1])))\n",
        "\n",
        "    def evaluate(self):\n",
        "        if self._value is None:\n",
        "            fn, args, kwargs = self._expr\n",
        "            \n",
        "            fn = fn.evaluate() if isinstance(fn, LazyValue) else fn\n",
        "            args = tuple(arg.evaluate() if isinstance(arg, LazyValue) else arg for arg in args)\n",
        "            kwargs = {k: v.evaluate() if isinstance(v, LazyValue) else v for k, v in kwargs.items()}\n",
        "            \n",
        "            self._value = fn(*args, **kwargs)\n",
        "        return self._value\n",
        "\n",
        "class LazyMessenger(pyro.poutine.messenger.Messenger):\n",
        "    def _process_message(self, msg):\n",
        "        if msg[\"type\"] in (\"apply\", \"sample\") and not msg[\"done\"]:\n",
        "            msg[\"done\"] = True\n",
        "            msg[\"value\"] = LazyValue(msg[\"fn\"], *msg[\"args\"], **msg[\"kwargs\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVJmLJC9AdSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@effectful(type=\"apply\")\n",
        "def add(x, y):\n",
        "    return x + y\n",
        "\n",
        "@effectful(type=\"apply\")\n",
        "def mul(x, y):\n",
        "    return x * y\n",
        "\n",
        "@effectful(type=\"apply\")\n",
        "def sigmoid(x):\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "@effectful(type=\"apply\")\n",
        "def normal(loc, scale):\n",
        "    return dist.Normal(loc, scale)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cYejlpddtho",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "56df2917-4abf-4d2a-d914-df9a323f58a7"
      },
      "source": [
        "# Applied to another model\n",
        "def biased_scale(guess):\n",
        "    weight = pyro.sample(\"weight\", normal(guess, 1.))\n",
        "    tolerance = pyro.sample(\"tolerance\", normal(0., 0.25))\n",
        "    return pyro.sample(\"measurement\", normal(add(mul(weight, 0.8), 1.), sigmoid(tolerance)))\n",
        "\n",
        "with LazyMessenger():\n",
        "    v = biased_scale(8.5)\n",
        "    print(v)\n",
        "    print(v.evaluate())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((<function normal at 0x7fca60dac8c8> (<function add at 0x7fca60dacf28> (<function mul at 0x7fca60dace18> ((<function normal at 0x7fca60dac8c8> 8.5 1.0) ) 0.8) 1.0) (<function sigmoid at 0x7fca60dacd08> ((<function normal at 0x7fca60dac8c8> 0.0 0.25) ))) )\n",
            "tensor(8.7122)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}