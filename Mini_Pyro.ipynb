{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mini_Pyro.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTIC3bxTXfXP",
        "colab_type": "text"
      },
      "source": [
        "# Mini Pyro Explanation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1705ZXFFXaUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "from collections import OrderedDict\n",
        "import weakref\n",
        "\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG1jQmeMEnaS",
        "colab_type": "text"
      },
      "source": [
        "## Pre example for weakref package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV4QpKabXru6",
        "colab_type": "code",
        "outputId": "740eddea-7b6f-49e6-9d96-42d7543f1e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# example comparing dict and WeakValueDictionary\n",
        "class C: pass\n",
        "ci=C()\n",
        "print(ci)\n",
        "\n",
        "wvd = weakref.WeakValueDictionary({'key' : ci})\n",
        "print(dict(wvd), len(wvd)) #1\n",
        "del ci\n",
        "print(dict(wvd), len(wvd)) #0\n",
        "\n",
        "ci2=C()\n",
        "d=dict()\n",
        "d['key']=ci2\n",
        "print(d, len(d))\n",
        "del ci2\n",
        "print(d, len(d))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.C object at 0x7fa3a53382b0>\n",
            "{'key': <__main__.C object at 0x7fa3a53382b0>} 1\n",
            "{} 0\n",
            "{'key': <__main__.C object at 0x7fa3a5338278>} 1\n",
            "{'key': <__main__.C object at 0x7fa3a5338278>} 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvuH4nbZL7gy",
        "colab_type": "text"
      },
      "source": [
        "## Pre example for **args* and **kwargs \n",
        "- Usage of *args\n",
        "  - They are mostly used in function definitions. **args* and **kwargs allow you to pass a variable number of arguments to a function. What variable means here is that you do not know beforehand how many arguments can be passed to your function by the user.\n",
        "- Usage of **kwargs\n",
        "  - ***kwargs allows you to pass keyworded variable length of arguments to a function. You should use **kwargs if you want to handle named arguments in a function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "webnThR7L8Lc",
        "colab_type": "code",
        "outputId": "1a6f24cd-a3e6-4ee7-ecbe-1b9daf4c01cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "def test_var_args(f_arg, *argv):\n",
        "    print(\"first normal arg:\", f_arg)\n",
        "    for arg in argv:\n",
        "        print(\"another arg through *argv:\", arg)\n",
        "test_var_args('yasoob', 'python', 'eggs', 'test')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first normal arg: yasoob\n",
            "another arg through *argv: python\n",
            "another arg through *argv: eggs\n",
            "another arg through *argv: test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMTKxKuwOL_c",
        "colab_type": "code",
        "outputId": "305422d2-9f52-4c6e-d7b8-6efd87db698c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def greet_me(**kwargs):\n",
        "    for key, value in kwargs.items():\n",
        "        print(\"{0} = {1}\".format(key, value))\n",
        "greet_me(name=\"yasoob\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name = yasoob\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdP3yIHjObun",
        "colab_type": "code",
        "outputId": "fa40026c-98d2-4828-ac82-81249440ec36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# combination\n",
        "def test_args_kwargs(arg1, arg2, arg3):\n",
        "    print(\"arg1:\", arg1)\n",
        "    print(\"arg2:\", arg2)\n",
        "    print(\"arg3:\", arg3)\n",
        "\n",
        "# first with *args\n",
        "args = (\"two\", 3, 5)\n",
        "test_args_kwargs(*args)\n",
        "\n",
        "# now with **kwargs:\n",
        "kwargs = {\"arg3\": 3, \"arg2\": \"two\", \"arg1\": 5}\n",
        "test_args_kwargs(**kwargs)\n",
        "\n",
        "# some_func(fargs, *args, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "arg1: two\n",
            "arg2: 3\n",
            "arg3: 5\n",
            "arg1: 5\n",
            "arg2: two\n",
            "arg3: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LwSBcR8vW2P",
        "colab_type": "text"
      },
      "source": [
        "## Pre example for super() function in self-defined class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFnfyCnBvWP6",
        "colab_type": "code",
        "outputId": "e5d84ad9-7df9-4717-c325-248d417ed4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "class Rectangle:\n",
        "    def __init__(self, length, width):\n",
        "        self.length = length\n",
        "        self.width = width\n",
        "\n",
        "    def area(self):\n",
        "        return self.length * self.width\n",
        "\n",
        "    def perimeter(self):\n",
        "        return 2 * self.length + 2 * self.width\n",
        "\n",
        "class Square(Rectangle):\n",
        "    def __init__(self, length):\n",
        "        # The first parameter refers to the subclass Square, \n",
        "        # while the second parameter refers to a Square object which, \n",
        "        # in this case, is self\n",
        "        super(Square, self).__init__(length, length)\n",
        "        \n",
        "# used super() to call the __init__() of the Rectangle class, \n",
        "# allowing you to use it in the Square class without repeating code\n",
        "# Rectangle is the superclass, and Square is the subclass\n",
        "square = Square(4)\n",
        "square.area()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrAZiuh8F8Lq",
        "colab_type": "text"
      },
      "source": [
        "## Initialize Pyro_stack and Param_store first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVaJ2TUHXrqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pyro keeps track of two kinds of global state:\n",
        "# i)  The effect handler stack, which enables non-standard interpretations of\n",
        "#     Pyro primitives like sample();\n",
        "#     See http://docs.pyro.ai/en/0.3.1/poutine.html\n",
        "# ii) Trainable parameters in the Pyro ParamStore;\n",
        "#     See http://docs.pyro.ai/en/0.3.1/parameters.html\n",
        "\n",
        "# Handlers earlier in the PYRO_STACK are applied first.\n",
        "PYRO_STACK = []\n",
        "PARAM_STORE = {}  # maps name -> (unconstrained_value, constraint)\n",
        "\n",
        "def get_param_store():\n",
        "    return PARAM_STORE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kqlVuGTBdU7",
        "colab_type": "text"
      },
      "source": [
        "## Messenger -- the very basic effect handler class in Pyro stack\n",
        "-  A generic Messenger actually contains two methods that are called once per operation where side effects are performed: \n",
        "  - 1. _process_message modifies a message and sends the result to the Messenger just above on the stack.\n",
        "  - 2. _postprocess_message modifies a message and sends the result to the next Messenger down on the stack. It is always called after all active Messengers have had their _process_message method applied to the message.\n",
        "\n",
        "- Although custom Messengers can override _process_message and _postprocess_message, it’s convenient to avoid requiring all effect handlers to be aware of all possible effectful operation types. For this reason, by default Messenger._process_message will use msg[\"type\"] to dispatch to a corresponding method Messenger._pyro_<type>, e.g. Messenger._pyro_sample as in LogJointMessenger. Just as exception handling code ignores unhandled exception types, this allows Messengers to simply forward operations they don’t know how to handle up to the next Messenger in the stack:\n",
        "- The order in which Messengers are applied to an operation like a pyro.sample statement is determined by the order in which their _ _enter__ methods are called. Messenger._ _enter__ appends a Messenger to the end (the bottom) of the global handler stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX41LA7jP9Jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An example for dispatching corresponding method\n",
        "class Messenger(object):\n",
        "    ...\n",
        "    def _process_message(self, msg):\n",
        "        method_name = \"_pyro_{}\".format(msg[\"type\"])  # e.g. _pyro_sample when msg[\"type\"] == \"sample\"\n",
        "        if hasattr(self, method_name):\n",
        "            getattr(self, method_name)(msg)\n",
        "    ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hoUJhjuA5cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The base effect handler class (called Messenger here for consistency with Pyro).\n",
        "class Messenger(object):\n",
        "    def __init__(self, fn=None):\n",
        "        self.fn = fn\n",
        "\n",
        "    # Effect handlers push themselves onto the PYRO_STACK.\n",
        "    # Handlers earlier in the PYRO_STACK are applied first.\n",
        "    \n",
        "    # Messenger.__enter__ appends a Messenger to the end (the bottom) of the global handler stack\n",
        "    def __enter__(self):\n",
        "        PYRO_STACK.append(self)\n",
        "        \n",
        "    # __exit__ removes a Messenger from the stack\n",
        "    # if the last messenger in the stack is itself then pop\n",
        "    def __exit__(self, *args, **kwargs):\n",
        "        assert PYRO_STACK[-1] is self\n",
        "        # remove the last element\n",
        "        PYRO_STACK.pop()\n",
        "\n",
        "    def process_message(self, msg):\n",
        "        pass\n",
        "    def postprocess_message(self, msg):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        with self:\n",
        "            return self.fn(*args, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-sDNosbA4lR",
        "colab_type": "text"
      },
      "source": [
        "## Some Messenger examples to show\n",
        "- _ _enter__ and _ _exit__ are special methods needed by any Python context manager. \n",
        "- When implementing new Messenger classes, if we override _ _enter__ and _ _exit__, we always need to call the base Messenger’s _ _enter__ and _ _exit__ methods for the new Messenger to be applied correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRnMrbzNctLs",
        "colab_type": "text"
      },
      "source": [
        "### 1. Trace Messenger - record the message info to a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me6xg8hxXrfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A first useful example of an effect handler.\n",
        "# trace records the inputs and outputs of any primitive site it encloses,\n",
        "# and returns a dictionary containing that data to the user.\n",
        "\n",
        "# trace class here can be considered as a kind of messenger\n",
        "# it inherits Messenger's enter function\n",
        "# define its own postprocess_message by storing info into a dictionary\n",
        "class trace(Messenger):\n",
        "    def __enter__(self):\n",
        "        super(trace, self).__enter__()\n",
        "        self.trace = OrderedDict()\n",
        "        return self.trace\n",
        "\n",
        "    # trace illustrates why we need postprocess_message in addition to process_message:\n",
        "    # We only want to record a value after all other effects have been applied\n",
        "    def postprocess_message(self, msg):\n",
        "        assert msg[\"name\"] not in self.trace, \"all sites must have unique names\"\n",
        "        self.trace[msg[\"name\"]] = msg.copy()\n",
        "\n",
        "    def get_trace(self, *args, **kwargs):\n",
        "        self(*args, **kwargs)\n",
        "        return self.trace"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByqNpPFQcbqX",
        "colab_type": "text"
      },
      "source": [
        "### 2. Replay Messenger - replace message value according to a trace dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__eWGvG5hh3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A second example of an effect handler for setting the value at a sample site.\n",
        "# This illustrates why effect handlers are a useful PPL implementation technique:\n",
        "# We can compose trace and replay to replace values but preserve distributions,\n",
        "# allowing us to compute the joint probability density of samples under a model.\n",
        "# See the definition of elbo(...) below for an example of this pattern.\n",
        "class replay(Messenger):\n",
        "    # guide_trace here is a dictionary\n",
        "    def __init__(self, fn, guide_trace):\n",
        "        self.guide_trace = guide_trace\n",
        "        super(replay, self).__init__(fn)\n",
        "\n",
        "    def process_message(self, msg):\n",
        "        # replace message value\n",
        "        if msg[\"name\"] in self.guide_trace:\n",
        "            msg[\"value\"] = self.guide_trace[msg[\"name\"]][\"value\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFOVQbcgeQrK",
        "colab_type": "text"
      },
      "source": [
        "### 3. Block Messenger - message would NOT be operated  by further handlers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP4lNc-qhh06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# block allows the selective application of effect handlers to different parts of a model.\n",
        "# Sites hidden by block will only have the handlers below block on the PYRO_STACK applied,\n",
        "# allowing inference or other effectful computations to be nested inside models.\n",
        "class block(Messenger):\n",
        "    def __init__(self, fn=None, hide_fn=lambda msg: True):\n",
        "        self.hide_fn = hide_fn\n",
        "        super(block, self).__init__(fn)\n",
        "\n",
        "    def process_message(self, msg):\n",
        "        if self.hide_fn(msg):\n",
        "            msg[\"stop\"] = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onm6VaAXhHx7",
        "colab_type": "text"
      },
      "source": [
        "### 4. Plate Messenger - generate conditionally independent samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9rS0WePhhx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This limited implementation of PlateMessenger only implements broadcasting.\n",
        "class PlateMessenger(Messenger):\n",
        "    def __init__(self, fn, size, dim):\n",
        "        assert dim < 0\n",
        "        self.size = size\n",
        "        self.dim = dim\n",
        "        super(PlateMessenger, self).__init__(fn)\n",
        "\n",
        "    def process_message(self, msg):\n",
        "        if msg[\"type\"] == \"sample\":\n",
        "            batch_shape = msg[\"fn\"].batch_shape\n",
        "            if len(batch_shape) < -self.dim or batch_shape[self.dim] != self.size:\n",
        "                batch_shape = [1] * (-self.dim - len(batch_shape)) + list(batch_shape)\n",
        "                batch_shape[self.dim] = self.size\n",
        "                msg[\"fn\"] = msg[\"fn\"].expand(torch.Size(batch_shape))\n",
        "\n",
        "    def __iter__(self):\n",
        "        return range(self.size)\n",
        "\n",
        "# boilerplate to match the syntax of actual pyro.plate:\n",
        "# Construct for conditionally independent sequences of variables\n",
        "def plate(name, size, dim):\n",
        "    return PlateMessenger(fn=None, size=size, dim=dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmmK3U3933-A",
        "colab_type": "text"
      },
      "source": [
        "##  Define sample and param function \n",
        "- If no active Messengers, we just draw a sample and return it as expected, otherwise initialize a message and call apply_stack function to send it to Messengers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAiHdlqahhvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample is an effectful version of Distribution.sample(...)\n",
        "# When any effect handlers are active, it constructs an initial message and calls apply_stack.\n",
        "# fn - distribution function for samples\n",
        "def sample(name, fn, obs=None):\n",
        "    # if there are no active Messengers, we just draw a sample and return it as expected:\n",
        "    if not PYRO_STACK:\n",
        "        return fn()\n",
        "    # Otherwise, we initialize a message...\n",
        "    initial_msg = {\n",
        "        \"type\": \"sample\",\n",
        "        \"name\": name,\n",
        "        \"fn\": fn,\n",
        "        \"args\": (),\n",
        "        \"value\": obs,\n",
        "    }\n",
        "    # ...and use apply_stack to send it to the Messengers\n",
        "    msg = apply_stack(initial_msg)\n",
        "    return msg[\"value\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdDjl1LshhqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# param is an effectful version of PARAM_STORE.setdefault that also handles constraints.\n",
        "# When any effect handlers are active, it constructs an initial message and calls apply_stack.\n",
        "def param(name, init_value=None, constraint=torch.distributions.constraints.real):\n",
        "\n",
        "    def fn(init_value, constraint):\n",
        "        # if exist already in param store then call directly\n",
        "        if name in PARAM_STORE:\n",
        "            unconstrained_value, constraint = PARAM_STORE[name]\n",
        "        else:\n",
        "            # Initialize with a constrained value.\n",
        "            assert init_value is not None\n",
        "            # \"with torch.no_grad()\" temporarily set all the requires_grad flag to false\n",
        "            with torch.no_grad():\n",
        "                # .detach() is to detach a tensor from the network graph, making the tensor no gradient\n",
        "                constrained_value = init_value.detach()\n",
        "                # The transform_to() registry is useful for performing unconstrained optimization \n",
        "                # on constrained parameters of probability distributions,\n",
        "                unconstrained_value = torch.distributions.transform_to(constraint).inv(constrained_value)\n",
        "            unconstrained_value.requires_grad_()\n",
        "            PARAM_STORE[name] = unconstrained_value, constraint\n",
        "\n",
        "        # Transform from unconstrained space to constrained space.\n",
        "        constrained_value = torch.distributions.transform_to(constraint)(unconstrained_value)\n",
        "        constrained_value.unconstrained = weakref.ref(unconstrained_value)\n",
        "        return constrained_value\n",
        "\n",
        "    # if there are no active Messengers, we just draw a sample and return it as expected:\n",
        "    if not PYRO_STACK:\n",
        "        return fn(init_value, constraint)\n",
        "    # Otherwise, we initialize a message...\n",
        "    initial_msg = {\n",
        "        \"type\": \"param\",\n",
        "        \"name\": name,\n",
        "        \"fn\": fn,\n",
        "        \"args\": (init_value, constraint),\n",
        "        \"value\": None,\n",
        "    }\n",
        "\n",
        "    # ...and use apply_stack to send it to the Messengers\n",
        "    msg = apply_stack(initial_msg)\n",
        "    return msg[\"value\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJEkksg3OY5N",
        "colab_type": "text"
      },
      "source": [
        "## Full initial message \n",
        "- The actual messages sent up and down the stack are dictionaries with a particular set of keys.Write out the full initial message here for completeness:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIewZgMwO_Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "msg = {\n",
        "    # The following fields contain the name, inputs, function, and output of a site.\n",
        "    # These are generally the only fields you'll need to think about.\n",
        "    \"name\": \"x\",\n",
        "    \"fn\": dist.Bernoulli(0.5),\n",
        "    \"value\": None,  # msg[\"value\"] will eventually contain the value returned by pyro.sample\n",
        "    \"is_observed\": False,  # because obs=None by default; only used by sample sites\n",
        "    \"args\": (),  # positional arguments passed to \"fn\" when it is called; usually empty for sample sites\n",
        "    \"kwargs\": {},  # keyword arguments passed to \"fn\" when it is called; usually empty for sample sites\n",
        "    # This field typically contains metadata needed or stored by a particular inference algorithm\n",
        "    \"infer\": {\"enumerate\": \"parallel\"},\n",
        "    # The remaining fields are generally only used by Pyro's internals,\n",
        "    # or for implementing more advanced effects beyond the scope of this tutorial\n",
        "    \"type\": \"sample\",  # label used by Messenger._process_message to dispatch, in this case to _pyro_sample\n",
        "    \"done\": False,\n",
        "    \"stop\": False,\n",
        "    \"scale\": torch.tensor(1.),  # Multiplicative scale factor that can be applied to each site's log_prob\n",
        "    \"mask\": None,\n",
        "    \"continuation\": None,\n",
        "    \"cond_indep_stack\": (),  # Will contain metadata from each pyro.plate enclosing this sample site.\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvos_XuVMQpF",
        "colab_type": "text"
      },
      "source": [
        "## apply_stack function - transfer message to stack for operations\n",
        "-  traverses the stack twice at each operation:\n",
        "  - first from bottom to top to apply each _process_message \n",
        "  - and then from top to bottom to apply each _postprocess_message"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMDjko_f4IBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply_stack is called by pyro.sample and pyro.param.\n",
        "# It is responsible for applying each Messenger to each effectful operation.\n",
        "def apply_stack(msg):\n",
        "    for pointer, handler in enumerate(reversed(PYRO_STACK)):\n",
        "        handler.process_message(msg)\n",
        "        # When a Messenger sets the \"stop\" field of a message,\n",
        "        # it prevents any Messengers above it on the stack from being applied.\n",
        "        if msg.get(\"stop\"):\n",
        "            break\n",
        "    if msg[\"value\"] is None:\n",
        "        # use args to run function and get message values\n",
        "        msg[\"value\"] = msg[\"fn\"](*msg[\"args\"])\n",
        "\n",
        "    # A Messenger that sets msg[\"stop\"] == True also prevents application\n",
        "    # of postprocess_message by Messengers above it on the stack\n",
        "    # via the pointer variable from the process_message loop\n",
        "    for handler in PYRO_STACK[-pointer-1:]:\n",
        "        handler.postprocess_message(msg)\n",
        "    return msg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLRLHtQ2B0jM",
        "colab_type": "text"
      },
      "source": [
        "## Adam optimizer class\n",
        "- It dynamically generates optimizers for dynamically generated parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zthksEDxhhoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a thin wrapper around the `torch.optim.Adam` class that\n",
        "# dynamically generates optimizers for dynamically generated parameters.\n",
        "# See http://docs.pyro.ai/en/0.3.1/optimization.html\n",
        "class Adam(object):\n",
        "    def __init__(self, optim_args):\n",
        "        self.optim_args = optim_args\n",
        "        # Each parameter will get its own optimizer, which we keep track\n",
        "        # of using this dictionary keyed on parameters.\n",
        "        self.optim_objs = {}\n",
        "\n",
        "    def __call__(self, params):\n",
        "        for param in params:\n",
        "            # If we've seen this parameter before, use the previously\n",
        "            # constructed optimizer.\n",
        "            if param in self.optim_objs:\n",
        "                optim = self.optim_objs[param]\n",
        "            # If we've never seen this parameter before, construct\n",
        "            # an Adam optimizer and keep track of it.\n",
        "            else:\n",
        "                optim = torch.optim.Adam([param], **self.optim_args)\n",
        "                self.optim_objs[param] = optim\n",
        "            # Take a gradient step for the parameter param.\n",
        "            optim.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWBdOhv3CEL0",
        "colab_type": "text"
      },
      "source": [
        "## SVI example - contain Messenger Trace and Block to store parameters values only\n",
        "- To be more specific: Here Trace Messenger comes first in the stack, then Block. Block here is put at the bottom but operates first (stack property) in the operating process. process__function sent the messege to the Messenger above(Trace) here then msg['stop'] == True thus stops. While if Trace Messenger works here, then it stores the msg info from Block Messenger successfully"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0GPJ2cBhhlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a unified interface for stochastic variational inference in Pyro.\n",
        "# The actual construction of the loss is taken care of by `loss`.\n",
        "# See http://docs.pyro.ai/en/0.3.1/inference_algos.html\n",
        "class SVI(object):\n",
        "    def __init__(self, model, guide, optim, loss):\n",
        "        self.model = model\n",
        "        self.guide = guide\n",
        "        self.optim = optim\n",
        "        self.loss = loss\n",
        "\n",
        "    # This method handles running the model and guide, constructing the loss\n",
        "    # function, and taking a gradient step.\n",
        "    def step(self, *args, **kwargs):\n",
        "        # This wraps both the call to `model` and `guide` in a `trace` so that\n",
        "        # we can record all the parameters that are encountered. Note that\n",
        "        # further tracing occurs inside of `loss`.\n",
        "        with trace() as param_capture:\n",
        "            # We use block here to allow tracing to record parameters only.\n",
        "            with block(hide_fn=lambda msg: msg[\"type\"] == \"sample\"):\n",
        "                loss = self.loss(self.model, self.guide, *args, **kwargs)\n",
        "        # Differentiate the loss.\n",
        "        loss.backward()\n",
        "        # Grab all the parameters from the trace.\n",
        "        params = [site[\"value\"].unconstrained()\n",
        "                  for site in param_capture.values()]\n",
        "        # Take a step w.r.t. each parameter in params.\n",
        "        self.optim(params)\n",
        "        # Zero out the gradients so that they don't accumulate.\n",
        "        for p in params:\n",
        "            p.grad = p.new_zeros(p.shape)\n",
        "        return loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcnFbIxqMGrc",
        "colab_type": "text"
      },
      "source": [
        "## ELBO calculation - using Trace Messenger to record and run the model\n",
        "- We’ve defined a Pyro model with observations x and latents z of the form pθ(x,z)=pθ(x|z)pθ(z). We’ve also defined a Pyro guide (i.e. a variational distribution) of the form qϕ(z). Here θ and ϕ are variational parameters for the model and guide, respectively. (In particular these are not random variables that call for a Bayesian treatment).\n",
        "- We’d like to maximize the log evidence logpθ(x) by maximizing the ELBO (the evidence lower bound) given by ELBO≡Eqϕ(z)[logpθ(x,z)−logqϕ(z)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2-TFUOfM4BJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one model example\n",
        "def model(data):\n",
        "    # define the hyperparameters that control the beta prior\n",
        "    alpha0 = torch.tensor(10.0)\n",
        "    beta0 = torch.tensor(10.0)\n",
        "    # sample f from the beta prior\n",
        "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
        "    # loop over the observed data\n",
        "    for i in range(len(data)):\n",
        "        # observe datapoint i using the bernoulli likelihood\n",
        "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
        "        \n",
        "# one guide example\n",
        "def guide(data):\n",
        "    # register the two variational parameters with Pyro.\n",
        "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0),\n",
        "                         constraint=constraints.positive)\n",
        "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0),\n",
        "                        constraint=constraints.positive)\n",
        "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
        "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n",
        "    \n",
        "# by running replay(model, guide_trace), latent_fairness would be \n",
        "# sampled from guide using alpha_q and beta_q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06ViIGkiinu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a basic implementation of the Evidence Lower Bound, which is the\n",
        "# fundamental objective in Variational Inference.\n",
        "# See http://pyro.ai/examples/svi_part_i.html for details.\n",
        "# This implementation has various limitations (for example it only supports\n",
        "# random variables with reparameterized samplers), but all the ELBO\n",
        "# implementations in Pyro share the same basic logic.\n",
        "def elbo(model, guide, *args, **kwargs):\n",
        "    # Run the guide with the arguments passed to SVI.step() and trace the execution,\n",
        "    # i.e. record all the calls to Pyro primitives like sample() and param().\n",
        "    guide_trace = trace(guide).get_trace(*args, **kwargs)\n",
        "    # Now run the model with the same arguments and trace the execution. Because\n",
        "    # model is being run with replay, whenever we encounter a sample site in the\n",
        "    # model, instead of sampling from the corresponding distribution in the model,\n",
        "    # we instead reuse the corresponding sample from the guide. In probabilistic\n",
        "    # terms, this means our loss is constructed as an expectation w.r.t. the joint\n",
        "    # distribution defined by the guide.\n",
        "    model_trace = trace(replay(model, guide_trace)).get_trace(*args, **kwargs)\n",
        "    # We will accumulate the various terms of the ELBO in `elbo`.\n",
        "    elbo = 0.\n",
        "    # Loop over all the sample sites in the model and add the corresponding\n",
        "    # log p(z) term to the ELBO. Note that this will also include any observed\n",
        "    # data, i.e. sample sites with the keyword `obs=...`.\n",
        "    for site in model_trace.values():\n",
        "        if site[\"type\"] == \"sample\":\n",
        "            elbo = elbo + site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
        "    # Loop over all the sample sites in the guide and add the corresponding\n",
        "    # -log q(z) term to the ELBO.\n",
        "    for site in guide_trace.values():\n",
        "        if site[\"type\"] == \"sample\":\n",
        "            elbo = elbo - site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
        "    # Return (-elbo) since by convention we do gradient descent on a loss and\n",
        "    # the ELBO is a lower bound that needs to be maximized.\n",
        "    return -elbo\n",
        "\n",
        "\n",
        "# This is a wrapper for compatibility with full Pyro.\n",
        "def Trace_ELBO(*args, **kwargs):\n",
        "    return elbo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA__3fQsRVTr",
        "colab_type": "text"
      },
      "source": [
        "## A final Example to show all above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntN6UDCxn9Y5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "outputId": "e16c4a41-8f8f-4b0d-f7a2-4e7cd52bd8f9"
      },
      "source": [
        "!pip install pyro-ppl"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyro-ppl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/e1/d67bf6252b9a0a1034bfd81c23fd28cdb8078670187f60084c1785bcae42/pyro-ppl-0.3.3.tar.gz (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.5.5)\n",
            "Requirement already satisfied: graphviz>=0.8 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.16.4)\n",
            "Collecting opt_einsum>=2.3.2 (from pyro-ppl)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 18.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.12.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.1.0)\n",
            "Collecting tqdm>=4.31 (from pyro-ppl)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/3d/7a6b68b631d2ab54975f3a4863f3c4e9b26445353264ef01f465dc9b0208/tqdm-4.32.2-py2.py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 17.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyro-ppl, opt-einsum\n",
            "  Building wheel for pyro-ppl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/6b/8b/8d15c6042ed38db155158baf56c1949a6e12d5d709697b0c37\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
            "Successfully built pyro-ppl opt-einsum\n",
            "Installing collected packages: opt-einsum, tqdm, pyro-ppl\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed opt-einsum-2.3.2 pyro-ppl-0.3.3 tqdm-4.32.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88VMl3SpShqz",
        "colab_type": "text"
      },
      "source": [
        "### Pre-example for Pyro.plate\n",
        "- Each invocation of plate requires the user to provide a unique name. The second argument is an integer just like for range.\n",
        "- Pyro can now leverage the conditional independency of the observations given the latent random variable. Basically pyro.plate is implemented using a context manager. At every execution of the body of the for loop we enter a new (conditional) independence context which is then exited at the end of the for loop body. \n",
        "  - because each observed pyro.sample statement occurs within a different execution of the body of the for loop, Pyro marks each observation as independent.\n",
        "  - this independence is properly a conditional independence given latent_fairness because latent_fairness is sampled outside of the context of data_loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK0eQFP3SiDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comparison between range() and pyro.plate\n",
        "def model(data):\n",
        "    # sample f from the beta prior\n",
        "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
        "    # loop over the observed data using pyro.sample with the obs keyword argument\n",
        "    for i in range(len(data)):\n",
        "        # observe datapoint i using the bernoulli likelihood\n",
        "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
        "        \n",
        "def model(data):\n",
        "    # sample f from the beta prior\n",
        "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
        "    # loop over the observed data [WE ONLY CHANGE THE NEXT LINE]\n",
        "    for i in pyro.plate(\"data_loop\", len(data)):\n",
        "        # observe datapoint i using the bernoulli likelihood\n",
        "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
        "\n",
        "# subsample minibatches of data\n",
        "with plate(\"data\", len(data), subsample_size=100) as ind:\n",
        "  batch = data[ind]\n",
        "  assert len(batch) == 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urMjXxsHinsL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "8128696e-39a6-4774-d48f-aea7692930c4"
      },
      "source": [
        "\"\"\"\n",
        "This example demonstrates the functionality of `pyro.contrib.minipyro`,\n",
        "which is a minimal implementation of the Pyro Probabilistic Programming\n",
        "Language that was created for didactic purposes.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import argparse\n",
        "import torch\n",
        "import pyro\n",
        "\n",
        "# We use the pyro.generic interface to support dynamic choice of backend.\n",
        "from pyro.generic import pyro_backend\n",
        "from pyro.generic import distributions as dist\n",
        "from pyro.generic import infer, optim, pyro\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # Define a basic model with a single Normal latent random variable `loc`\n",
        "    # and a batch of Normally distributed observations.\n",
        "    def model(data):\n",
        "        loc = pyro.sample(\"loc\", dist.Normal(0., 1.))\n",
        "        with pyro.plate(\"data\", len(data), dim=-1):\n",
        "            pyro.sample(\"obs\", dist.Normal(loc, 1.), obs=data)\n",
        "\n",
        "    # Define a guide (i.e. variational distribution) with a Normal\n",
        "    # distribution over the latent random variable `loc`.\n",
        "    def guide(data):\n",
        "        guide_loc = pyro.param(\"guide_loc\", torch.tensor(0.))\n",
        "        guide_scale = pyro.param(\"guide_scale_log\", torch.tensor(0.)).exp()\n",
        "        pyro.sample(\"loc\", dist.Normal(guide_loc, guide_scale))\n",
        "\n",
        "    # Generate some data.\n",
        "    torch.manual_seed(0)\n",
        "    data = torch.randn(100) + 3.0\n",
        "\n",
        "    # Because the API in minipyro matches that of Pyro proper,\n",
        "    # training code works with generic Pyro implementations.\n",
        "    with pyro_backend(args[\"backend\"]):\n",
        "        # Construct an SVI object so we can do variational inference on our\n",
        "        # model/guide pair.\n",
        "        elbo = infer.Trace_ELBO()\n",
        "        adam = optim.Adam({\"lr\": args[\"learning_rate\"]})\n",
        "        svi = infer.SVI(model, guide, adam, elbo)\n",
        "\n",
        "        # Basic training loop\n",
        "        pyro.get_param_store().clear()\n",
        "        for step in range(args[\"num_steps\"]):\n",
        "            loss = svi.step(data)\n",
        "            if step % 100 == 0:\n",
        "                print(\"step {} loss = {}\".format(step, loss))\n",
        "\n",
        "        # Report the final values of the variational parameters\n",
        "        # in the guide after training.\n",
        "        for name in pyro.get_param_store():\n",
        "            value = pyro.param(name)\n",
        "            print(\"{} = {}\".format(name, value.detach().cpu().numpy()))\n",
        "\n",
        "        # For this simple (conjugate) model we know the exact posterior. In\n",
        "        # particular we know that the variational distribution should be\n",
        "        # centered near 3.0. So let's check this explicitly.\n",
        "        assert (pyro.param(\"guide_loc\") - 3.0).abs() < 0.1\n",
        "\n",
        "args = {\"num_steps\": 1001, \"learning_rate\": 0.02, \"backend\": \"minipyro\"}\n",
        "main(args)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     assert pyro.__version__.startswith('0.3.3')\n",
        "#     parser = argparse.ArgumentParser(description=\"Mini Pyro demo\")\n",
        "#     parser.add_argument(\"-b\", \"--backend\", default=\"minipyro\")\n",
        "#     parser.add_argument(\"-n\", \"--num-steps\", default=1001, type=int)\n",
        "#     parser.add_argument(\"-lr\", \"--learning-rate\", default=0.02, type=float)\n",
        "#     args = parser.parse_args()\n",
        "#     main(args)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0 loss = 291.2471618652344\n",
            "step 100 loss = 164.15792846679688\n",
            "step 200 loss = 149.47970581054688\n",
            "step 300 loss = 150.03028869628906\n",
            "step 400 loss = 165.29713439941406\n",
            "step 500 loss = 153.3885955810547\n",
            "step 600 loss = 164.81736755371094\n",
            "step 700 loss = 150.8622589111328\n",
            "step 800 loss = 150.74578857421875\n",
            "step 900 loss = 150.77191162109375\n",
            "step 1000 loss = 152.4605712890625\n",
            "guide_loc = 3.0301661491394043\n",
            "guide_scale_log = -1.7844144105911255\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}