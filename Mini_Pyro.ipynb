{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mini_Pyro.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTIC3bxTXfXP",
        "colab_type": "text"
      },
      "source": [
        "# Mini Pyro Explanation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1705ZXFFXaUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "from collections import OrderedDict\n",
        "import weakref\n",
        "\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG1jQmeMEnaS",
        "colab_type": "text"
      },
      "source": [
        "## Pre example for weakref package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV4QpKabXru6",
        "colab_type": "code",
        "outputId": "740eddea-7b6f-49e6-9d96-42d7543f1e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# example comparing dict and WeakValueDictionary\n",
        "class C: pass\n",
        "ci=C()\n",
        "print(ci)\n",
        "\n",
        "wvd = weakref.WeakValueDictionary({'key' : ci})\n",
        "print(dict(wvd), len(wvd)) #1\n",
        "del ci\n",
        "print(dict(wvd), len(wvd)) #0\n",
        "\n",
        "ci2=C()\n",
        "d=dict()\n",
        "d['key']=ci2\n",
        "print(d, len(d))\n",
        "del ci2\n",
        "print(d, len(d))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.C object at 0x7fa3a53382b0>\n",
            "{'key': <__main__.C object at 0x7fa3a53382b0>} 1\n",
            "{} 0\n",
            "{'key': <__main__.C object at 0x7fa3a5338278>} 1\n",
            "{'key': <__main__.C object at 0x7fa3a5338278>} 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvuH4nbZL7gy",
        "colab_type": "text"
      },
      "source": [
        "## Pre example for **args* and **kwargs \n",
        "- Usage of *args\n",
        "  - They are mostly used in function definitions. **args* and **kwargs allow you to pass a variable number of arguments to a function. What variable means here is that you do not know beforehand how many arguments can be passed to your function by the user.\n",
        "- Usage of **kwargs\n",
        "  - ***kwargs allows you to pass keyworded variable length of arguments to a function. You should use **kwargs if you want to handle named arguments in a function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "webnThR7L8Lc",
        "colab_type": "code",
        "outputId": "1a6f24cd-a3e6-4ee7-ecbe-1b9daf4c01cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "def test_var_args(f_arg, *argv):\n",
        "    print(\"first normal arg:\", f_arg)\n",
        "    for arg in argv:\n",
        "        print(\"another arg through *argv:\", arg)\n",
        "test_var_args('yasoob', 'python', 'eggs', 'test')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first normal arg: yasoob\n",
            "another arg through *argv: python\n",
            "another arg through *argv: eggs\n",
            "another arg through *argv: test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMTKxKuwOL_c",
        "colab_type": "code",
        "outputId": "305422d2-9f52-4c6e-d7b8-6efd87db698c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def greet_me(**kwargs):\n",
        "    for key, value in kwargs.items():\n",
        "        print(\"{0} = {1}\".format(key, value))\n",
        "greet_me(name=\"yasoob\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name = yasoob\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdP3yIHjObun",
        "colab_type": "code",
        "outputId": "fa40026c-98d2-4828-ac82-81249440ec36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# combination\n",
        "def test_args_kwargs(arg1, arg2, arg3):\n",
        "    print(\"arg1:\", arg1)\n",
        "    print(\"arg2:\", arg2)\n",
        "    print(\"arg3:\", arg3)\n",
        "\n",
        "# first with *args\n",
        "args = (\"two\", 3, 5)\n",
        "test_args_kwargs(*args)\n",
        "\n",
        "# now with **kwargs:\n",
        "kwargs = {\"arg3\": 3, \"arg2\": \"two\", \"arg1\": 5}\n",
        "test_args_kwargs(**kwargs)\n",
        "\n",
        "# some_func(fargs, *args, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "arg1: two\n",
            "arg2: 3\n",
            "arg3: 5\n",
            "arg1: 5\n",
            "arg2: two\n",
            "arg3: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LwSBcR8vW2P",
        "colab_type": "text"
      },
      "source": [
        "## Pre example for super() function in self-defined class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFnfyCnBvWP6",
        "colab_type": "code",
        "outputId": "e5d84ad9-7df9-4717-c325-248d417ed4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "class Rectangle:\n",
        "    def __init__(self, length, width):\n",
        "        self.length = length\n",
        "        self.width = width\n",
        "\n",
        "    def area(self):\n",
        "        return self.length * self.width\n",
        "\n",
        "    def perimeter(self):\n",
        "        return 2 * self.length + 2 * self.width\n",
        "\n",
        "class Square(Rectangle):\n",
        "    def __init__(self, length):\n",
        "        # The first parameter refers to the subclass Square, \n",
        "        # while the second parameter refers to a Square object which, \n",
        "        # in this case, is self\n",
        "        super(Square, self).__init__(length, length)\n",
        "        \n",
        "# used super() to call the __init__() of the Rectangle class, \n",
        "# allowing you to use it in the Square class without repeating code\n",
        "# Rectangle is the superclass, and Square is the subclass\n",
        "square = Square(4)\n",
        "square.area()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrAZiuh8F8Lq",
        "colab_type": "text"
      },
      "source": [
        "## Initialize Pyro_stack and Param_store first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVaJ2TUHXrqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pyro keeps track of two kinds of global state:\n",
        "# i)  The effect handler stack, which enables non-standard interpretations of\n",
        "#     Pyro primitives like sample();\n",
        "#     See http://docs.pyro.ai/en/0.3.1/poutine.html\n",
        "# ii) Trainable parameters in the Pyro ParamStore;\n",
        "#     See http://docs.pyro.ai/en/0.3.1/parameters.html\n",
        "\n",
        "# Handlers earlier in the PYRO_STACK are applied first.\n",
        "PYRO_STACK = []\n",
        "PARAM_STORE = {}  # maps name -> (unconstrained_value, constraint)\n",
        "\n",
        "def get_param_store():\n",
        "    return PARAM_STORE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kqlVuGTBdU7",
        "colab_type": "text"
      },
      "source": [
        "## Messenger -- the very basic effect handler class in Pyro stack\n",
        "-  A generic Messenger actually contains two methods that are called once per operation where side effects are performed: \n",
        "  - 1. _process_message modifies a message and sends the result to the Messenger just above on the stack.\n",
        "  - 2. _postprocess_message modifies a message and sends the result to the next Messenger down on the stack. It is always called after all active Messengers have had their _process_message method applied to the message.\n",
        "\n",
        "- Although custom Messengers can override _process_message and _postprocess_message, itâ€™s convenient to avoid requiring all effect handlers to be aware of all possible effectful operation types. For this reason, by default Messenger._process_message will use msg[\"type\"] to dispatch to a corresponding method Messenger._pyro_<type>, e.g. Messenger._pyro_sample as in LogJointMessenger. Just as exception handling code ignores unhandled exception types, this allows Messengers to simply forward operations they donâ€™t know how to handle up to the next Messenger in the stack:\n",
        "- The order in which Messengers are applied to an operation like a pyro.sample statement is determined by the order in which their _ _enter__ methods are called. Messenger._ _enter__ appends a Messenger to the end (the bottom) of the global handler stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX41LA7jP9Jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An example for dispatching corresponding method\n",
        "class Messenger(object):\n",
        "    ...\n",
        "    def _process_message(self, msg):\n",
        "        method_name = \"_pyro_{}\".format(msg[\"type\"])  # e.g. _pyro_sample when msg[\"type\"] == \"sample\"\n",
        "        if hasattr(self, method_name):\n",
        "            getattr(self, method_name)(msg)\n",
        "    ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hoUJhjuA5cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The base effect handler class (called Messenger here for consistency with Pyro).\n",
        "class Messenger(object):\n",
        "    def __init__(self, fn=None):\n",
        "        self.fn = fn\n",
        "\n",
        "    # Effect handlers push themselves onto the PYRO_STACK.\n",
        "    # Handlers earlier in the PYRO_STACK are applied first.\n",
        "    \n",
        "    # Messenger.__enter__ appends a Messenger to the end (the bottom) of the global handler stack\n",
        "    def __enter__(self):\n",
        "        PYRO_STACK.append(self)\n",
        "        \n",
        "    # __exit__ removes a Messenger from the stack\n",
        "    # if the last messenger in the stack is itself then pop\n",
        "    def __exit__(self, *args, **kwargs):\n",
        "        assert PYRO_STACK[-1] is self\n",
        "        # remove the last element\n",
        "        PYRO_STACK.pop()\n",
        "\n",
        "    def process_message(self, msg):\n",
        "        pass\n",
        "    def postprocess_message(self, msg):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        with self:\n",
        "            return self.fn(*args, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-sDNosbA4lR",
        "colab_type": "text"
      },
      "source": [
        "## Some Messenger examples to show\n",
        "- _ _enter__ and _ _exit__ are special methods needed by any Python context manager. \n",
        "- When implementing new Messenger classes, if we override _ _enter__ and _ _exit__, we always need to call the base Messengerâ€™s _ _enter__ and _ _exit__ methods for the new Messenger to be applied correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRnMrbzNctLs",
        "colab_type": "text"
      },
      "source": [
        "### 1. Trace Messenger - record the message info to a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me6xg8hxXrfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A first useful example of an effect handler.\n",
        "# trace records the inputs and outputs of any primitive site it encloses,\n",
        "# and returns a dictionary containing that data to the user.\n",
        "\n",
        "# trace class here can be considered as a kind of messenger\n",
        "# it inherits Messenger's enter function\n",
        "# define its own postprocess_message by storing info into a dictionary\n",
        "class trace(Messenger):\n",
        "    def __enter__(self):\n",
        "        super(trace, self).__enter__()\n",
        "        self.trace = OrderedDict()\n",
        "        return self.trace\n",
        "\n",
        "    # trace illustrates why we need postprocess_message in addition to process_message:\n",
        "    # We only want to record a value after all other effects have been applied\n",
        "    def postprocess_message(self, msg):\n",
        "        assert msg[\"name\"] not in self.trace, \"all sites must have unique names\"\n",
        "        self.trace[msg[\"name\"]] = msg.copy()\n",
        "\n",
        "    def get_trace(self, *args, **kwargs):\n",
        "        self(*args, **kwargs)\n",
        "        return self.trace"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByqNpPFQcbqX",
        "colab_type": "text"
      },
      "source": [
        "### 2. Replay Messenger - replace message value according to a trace dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__eWGvG5hh3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A second example of an effect handler for setting the value at a sample site.\n",
        "# This illustrates why effect handlers are a useful PPL implementation technique:\n",
        "# We can compose trace and replay to replace values but preserve distributions,\n",
        "# allowing us to compute the joint probability density of samples under a model.\n",
        "# See the definition of elbo(...) below for an example of this pattern.\n",
        "class replay(Messenger):\n",
        "    # guide_trace here is a dictionary\n",
        "    def __init__(self, fn, guide_trace):\n",
        "        self.guide_trace = guide_trace\n",
        "        super(replay, self).__init__(fn)\n",
        "\n",
        "    def process_message(self, msg):\n",
        "        # replace message value\n",
        "        if msg[\"name\"] in self.guide_trace:\n",
        "            msg[\"value\"] = self.guide_trace[msg[\"name\"]][\"value\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFOVQbcgeQrK",
        "colab_type": "text"
      },
      "source": [
        "### 3. Block Messenger - message would NOT be operated  by further handlers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP4lNc-qhh06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# block allows the selective application of effect handlers to different parts of a model.\n",
        "# Sites hidden by block will only have the handlers below block on the PYRO_STACK applied,\n",
        "# allowing inference or other effectful computations to be nested inside models.\n",
        "class block(Messenger):\n",
        "    def __init__(self, fn=None, hide_fn=lambda msg: True):\n",
        "        self.hide_fn = hide_fn\n",
        "        super(block, self).__init__(fn)\n",
        "\n",
        "    def process_message(self, msg):\n",
        "        if self.hide_fn(msg):\n",
        "            msg[\"stop\"] = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onm6VaAXhHx7",
        "colab_type": "text"
      },
      "source": [
        "### 4. Plate Messenger - generate conditionally independent samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9rS0WePhhx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This limited implementation of PlateMessenger only implements broadcasting.\n",
        "class PlateMessenger(Messenger):\n",
        "    def __init__(self, fn, size, dim):\n",
        "        assert dim < 0\n",
        "        self.size = size\n",
        "        self.dim = dim\n",
        "        super(PlateMessenger, self).__init__(fn)\n",
        "\n",
        "    def process_message(self, msg):\n",
        "        if msg[\"type\"] == \"sample\":\n",
        "            batch_shape = msg[\"fn\"].batch_shape\n",
        "            if len(batch_shape) < -self.dim or batch_shape[self.dim] != self.size:\n",
        "                batch_shape = [1] * (-self.dim - len(batch_shape)) + list(batch_shape)\n",
        "                batch_shape[self.dim] = self.size\n",
        "                msg[\"fn\"] = msg[\"fn\"].expand(torch.Size(batch_shape))\n",
        "\n",
        "    def __iter__(self):\n",
        "        return range(self.size)\n",
        "\n",
        "# boilerplate to match the syntax of actual pyro.plate:\n",
        "# Construct for conditionally independent sequences of variables\n",
        "def plate(name, size, dim):\n",
        "    return PlateMessenger(fn=None, size=size, dim=dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmmK3U3933-A",
        "colab_type": "text"
      },
      "source": [
        "##  Define sample and param function \n",
        "- If no active Messengers, we just draw a sample and return it as expected, otherwise initialize a message and call apply_stack function to send it to Messengers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAiHdlqahhvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample is an effectful version of Distribution.sample(...)\n",
        "# When any effect handlers are active, it constructs an initial message and calls apply_stack.\n",
        "# fn - distribution function for samples\n",
        "def sample(name, fn, obs=None):\n",
        "    # if there are no active Messengers, we just draw a sample and return it as expected:\n",
        "    if not PYRO_STACK:\n",
        "        return fn()\n",
        "    # Otherwise, we initialize a message...\n",
        "    initial_msg = {\n",
        "        \"type\": \"sample\",\n",
        "        \"name\": name,\n",
        "        \"fn\": fn,\n",
        "        \"args\": (),\n",
        "        \"value\": obs,\n",
        "    }\n",
        "    # ...and use apply_stack to send it to the Messengers\n",
        "    msg = apply_stack(initial_msg)\n",
        "    return msg[\"value\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdDjl1LshhqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# param is an effectful version of PARAM_STORE.setdefault that also handles constraints.\n",
        "# When any effect handlers are active, it constructs an initial message and calls apply_stack.\n",
        "def param(name, init_value=None, constraint=torch.distributions.constraints.real):\n",
        "\n",
        "    def fn(init_value, constraint):\n",
        "        # if exist already in param store then call directly\n",
        "        if name in PARAM_STORE:\n",
        "            unconstrained_value, constraint = PARAM_STORE[name]\n",
        "        else:\n",
        "            # Initialize with a constrained value.\n",
        "            assert init_value is not None\n",
        "            # \"with torch.no_grad()\" temporarily set all the requires_grad flag to false\n",
        "            with torch.no_grad():\n",
        "                # .detach() is to detach a tensor from the network graph, making the tensor no gradient\n",
        "                constrained_value = init_value.detach()\n",
        "                # The transform_to() registry is useful for performing unconstrained optimization \n",
        "                # on constrained parameters of probability distributions,\n",
        "                unconstrained_value = torch.distributions.transform_to(constraint).inv(constrained_value)\n",
        "            unconstrained_value.requires_grad_()\n",
        "            PARAM_STORE[name] = unconstrained_value, constraint\n",
        "\n",
        "        # Transform from unconstrained space to constrained space.\n",
        "        constrained_value = torch.distributions.transform_to(constraint)(unconstrained_value)\n",
        "        constrained_value.unconstrained = weakref.ref(unconstrained_value)\n",
        "        return constrained_value\n",
        "\n",
        "    # if there are no active Messengers, we just draw a sample and return it as expected:\n",
        "    if not PYRO_STACK:\n",
        "        return fn(init_value, constraint)\n",
        "    # Otherwise, we initialize a message...\n",
        "    initial_msg = {\n",
        "        \"type\": \"param\",\n",
        "        \"name\": name,\n",
        "        \"fn\": fn,\n",
        "        \"args\": (init_value, constraint),\n",
        "        \"value\": None,\n",
        "    }\n",
        "\n",
        "    # ...and use apply_stack to send it to the Messengers\n",
        "    msg = apply_stack(initial_msg)\n",
        "    return msg[\"value\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJEkksg3OY5N",
        "colab_type": "text"
      },
      "source": [
        "## Full initial message \n",
        "- The actual messages sent up and down the stack are dictionaries with a particular set of keys.Write out the full initial message here for completeness:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIewZgMwO_Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "msg = {\n",
        "    # The following fields contain the name, inputs, function, and output of a site.\n",
        "    # These are generally the only fields you'll need to think about.\n",
        "    \"name\": \"x\",\n",
        "    \"fn\": dist.Bernoulli(0.5),\n",
        "    \"value\": None,  # msg[\"value\"] will eventually contain the value returned by pyro.sample\n",
        "    \"is_observed\": False,  # because obs=None by default; only used by sample sites\n",
        "    \"args\": (),  # positional arguments passed to \"fn\" when it is called; usually empty for sample sites\n",
        "    \"kwargs\": {},  # keyword arguments passed to \"fn\" when it is called; usually empty for sample sites\n",
        "    # This field typically contains metadata needed or stored by a particular inference algorithm\n",
        "    \"infer\": {\"enumerate\": \"parallel\"},\n",
        "    # The remaining fields are generally only used by Pyro's internals,\n",
        "    # or for implementing more advanced effects beyond the scope of this tutorial\n",
        "    \"type\": \"sample\",  # label used by Messenger._process_message to dispatch, in this case to _pyro_sample\n",
        "    \"done\": False,\n",
        "    \"stop\": False,\n",
        "    \"scale\": torch.tensor(1.),  # Multiplicative scale factor that can be applied to each site's log_prob\n",
        "    \"mask\": None,\n",
        "    \"continuation\": None,\n",
        "    \"cond_indep_stack\": (),  # Will contain metadata from each pyro.plate enclosing this sample site.\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvos_XuVMQpF",
        "colab_type": "text"
      },
      "source": [
        "## apply_stack function - transfer message to stack for operations\n",
        "-  traverses the stack twice at each operation:\n",
        "  - first from bottom to top to apply each _process_message \n",
        "  - and then from top to bottom to apply each _postprocess_message"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMDjko_f4IBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply_stack is called by pyro.sample and pyro.param.\n",
        "# It is responsible for applying each Messenger to each effectful operation.\n",
        "def apply_stack(msg):\n",
        "    for pointer, handler in enumerate(reversed(PYRO_STACK)):\n",
        "        handler.process_message(msg)\n",
        "        # When a Messenger sets the \"stop\" field of a message,\n",
        "        # it prevents any Messengers above it on the stack from being applied.\n",
        "        if msg.get(\"stop\"):\n",
        "            break\n",
        "    if msg[\"value\"] is None:\n",
        "        # use args to run function and get message values\n",
        "        msg[\"value\"] = msg[\"fn\"](*msg[\"args\"])\n",
        "\n",
        "    # A Messenger that sets msg[\"stop\"] == True also prevents application\n",
        "    # of postprocess_message by Messengers above it on the stack\n",
        "    # via the pointer variable from the process_message loop\n",
        "    for handler in PYRO_STACK[-pointer-1:]:\n",
        "        handler.postprocess_message(msg)\n",
        "    return msg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLRLHtQ2B0jM",
        "colab_type": "text"
      },
      "source": [
        "## Adam optimizer class\n",
        "- It dynamically generates optimizers for dynamically generated parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zthksEDxhhoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a thin wrapper around the `torch.optim.Adam` class that\n",
        "# dynamically generates optimizers for dynamically generated parameters.\n",
        "# See http://docs.pyro.ai/en/0.3.1/optimization.html\n",
        "class Adam(object):\n",
        "    def __init__(self, optim_args):\n",
        "        self.optim_args = optim_args\n",
        "        # Each parameter will get its own optimizer, which we keep track\n",
        "        # of using this dictionary keyed on parameters.\n",
        "        self.optim_objs = {}\n",
        "\n",
        "    def __call__(self, params):\n",
        "        for param in params:\n",
        "            # If we've seen this parameter before, use the previously\n",
        "            # constructed optimizer.\n",
        "            if param in self.optim_objs:\n",
        "                optim = self.optim_objs[param]\n",
        "            # If we've never seen this parameter before, construct\n",
        "            # an Adam optimizer and keep track of it.\n",
        "            else:\n",
        "                optim = torch.optim.Adam([param], **self.optim_args)\n",
        "                self.optim_objs[param] = optim\n",
        "            # Take a gradient step for the parameter param.\n",
        "            optim.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWBdOhv3CEL0",
        "colab_type": "text"
      },
      "source": [
        "## SVI example - contain Messenger Trace and Block to store parameters values only\n",
        "- To be more specific: Here Trace Messenger comes first in the stack, then Block. Block here is put at the bottom but operates first (stack property) in the operating process. process__function sent the messege to the Messenger above(Trace) here then msg['stop'] == True thus stops. While if Trace Messenger works here, then it stores the msg info from Block Messenger successfully"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0GPJ2cBhhlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a unified interface for stochastic variational inference in Pyro.\n",
        "# The actual construction of the loss is taken care of by `loss`.\n",
        "# See http://docs.pyro.ai/en/0.3.1/inference_algos.html\n",
        "class SVI(object):\n",
        "    def __init__(self, model, guide, optim, loss):\n",
        "        self.model = model\n",
        "        self.guide = guide\n",
        "        self.optim = optim\n",
        "        self.loss = loss\n",
        "\n",
        "    # This method handles running the model and guide, constructing the loss\n",
        "    # function, and taking a gradient step.\n",
        "    def step(self, *args, **kwargs):\n",
        "        # This wraps both the call to `model` and `guide` in a `trace` so that\n",
        "        # we can record all the parameters that are encountered. Note that\n",
        "        # further tracing occurs inside of `loss`.\n",
        "        with trace() as param_capture:\n",
        "            # We use block here to allow tracing to record parameters only.\n",
        "            with block(hide_fn=lambda msg: msg[\"type\"] == \"sample\"):\n",
        "                loss = self.loss(self.model, self.guide, *args, **kwargs)\n",
        "        # Differentiate the loss.\n",
        "        loss.backward()\n",
        "        # Grab all the parameters from the trace.\n",
        "        params = [site[\"value\"].unconstrained()\n",
        "                  for site in param_capture.values()]\n",
        "        # Take a step w.r.t. each parameter in params.\n",
        "        self.optim(params)\n",
        "        # Zero out the gradients so that they don't accumulate.\n",
        "        for p in params:\n",
        "            p.grad = p.new_zeros(p.shape)\n",
        "        return loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcnFbIxqMGrc",
        "colab_type": "text"
      },
      "source": [
        "## ELBO calculation - using Trace Messenger to record and run the model\n",
        "- Weâ€™ve defined a Pyro model with observations x and latents z of the form pÎ¸(x,z)=pÎ¸(x|z)pÎ¸(z). Weâ€™ve also defined a Pyro guide (i.e. a variational distribution) of the form qÏ•(z). Here Î¸ and Ï• are variational parameters for the model and guide, respectively. (In particular these are not random variables that call for a Bayesian treatment).\n",
        "- Weâ€™d like to maximize the log evidence logpÎ¸(x) by maximizing the ELBO (the evidence lower bound) given by ELBOâ‰¡EqÏ•(z)[logpÎ¸(x,z)âˆ’logqÏ•(z)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2-TFUOfM4BJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one model example\n",
        "def model(data):\n",
        "    # define the hyperparameters that control the beta prior\n",
        "    alpha0 = torch.tensor(10.0)\n",
        "    beta0 = torch.tensor(10.0)\n",
        "    # sample f from the beta prior\n",
        "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
        "    # loop over the observed data\n",
        "    for i in range(len(data)):\n",
        "        # observe datapoint i using the bernoulli likelihood\n",
        "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
        "        \n",
        "# one guide example\n",
        "def guide(data):\n",
        "    # register the two variational parameters with Pyro.\n",
        "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0),\n",
        "                         constraint=constraints.positive)\n",
        "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0),\n",
        "                        constraint=constraints.positive)\n",
        "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
        "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n",
        "    \n",
        "# by running replay(model, guide_trace), latent_fairness would be \n",
        "# sampled from guide using alpha_q and beta_q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06ViIGkiinu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a basic implementation of the Evidence Lower Bound, which is the\n",
        "# fundamental objective in Variational Inference.\n",
        "# See http://pyro.ai/examples/svi_part_i.html for details.\n",
        "# This implementation has various limitations (for example it only supports\n",
        "# random variables with reparameterized samplers), but all the ELBO\n",
        "# implementations in Pyro share the same basic logic.\n",
        "def elbo(model, guide, *args, **kwargs):\n",
        "    # Run the guide with the arguments passed to SVI.step() and trace the execution,\n",
        "    # i.e. record all the calls to Pyro primitives like sample() and param().\n",
        "    guide_trace = trace(guide).get_trace(*args, **kwargs)\n",
        "    # Now run the model with the same arguments and trace the execution. Because\n",
        "    # model is being run with replay, whenever we encounter a sample site in the\n",
        "    # model, instead of sampling from the corresponding distribution in the model,\n",
        "    # we instead reuse the corresponding sample from the guide. In probabilistic\n",
        "    # terms, this means our loss is constructed as an expectation w.r.t. the joint\n",
        "    # distribution defined by the guide.\n",
        "    model_trace = trace(replay(model, guide_trace)).get_trace(*args, **kwargs)\n",
        "    # We will accumulate the various terms of the ELBO in `elbo`.\n",
        "    elbo = 0.\n",
        "    # Loop over all the sample sites in the model and add the corresponding\n",
        "    # log p(z) term to the ELBO. Note that this will also include any observed\n",
        "    # data, i.e. sample sites with the keyword `obs=...`.\n",
        "    for site in model_trace.values():\n",
        "        if site[\"type\"] == \"sample\":\n",
        "            elbo = elbo + site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
        "    # Loop over all the sample sites in the guide and add the corresponding\n",
        "    # -log q(z) term to the ELBO.\n",
        "    for site in guide_trace.values():\n",
        "        if site[\"type\"] == \"sample\":\n",
        "            elbo = elbo - site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
        "    # Return (-elbo) since by convention we do gradient descent on a loss and\n",
        "    # the ELBO is a lower bound that needs to be maximized.\n",
        "    return -elbo\n",
        "\n",
        "\n",
        "# This is a wrapper for compatibility with full Pyro.\n",
        "def Trace_ELBO(*args, **kwargs):\n",
        "    return elbo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA__3fQsRVTr",
        "colab_type": "text"
      },
      "source": [
        "## A final Example to show all above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntN6UDCxn9Y5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "outputId": "e16c4a41-8f8f-4b0d-f7a2-4e7cd52bd8f9"
      },
      "source": [
        "!pip install pyro-ppl"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyro-ppl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/e1/d67bf6252b9a0a1034bfd81c23fd28cdb8078670187f60084c1785bcae42/pyro-ppl-0.3.3.tar.gz (231kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.5.5)\n",
            "Requirement already satisfied: graphviz>=0.8 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.16.4)\n",
            "Collecting opt_einsum>=2.3.2 (from pyro-ppl)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 18.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.12.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.1.0)\n",
            "Collecting tqdm>=4.31 (from pyro-ppl)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/3d/7a6b68b631d2ab54975f3a4863f3c4e9b26445353264ef01f465dc9b0208/tqdm-4.32.2-py2.py3-none-any.whl (50kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 17.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyro-ppl, opt-einsum\n",
            "  Building wheel for pyro-ppl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/6b/8b/8d15c6042ed38db155158baf56c1949a6e12d5d709697b0c37\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
            "Successfully built pyro-ppl opt-einsum\n",
            "Installing collected packages: opt-einsum, tqdm, pyro-ppl\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed opt-einsum-2.3.2 pyro-ppl-0.3.3 tqdm-4.32.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88VMl3SpShqz",
        "colab_type": "text"
      },
      "source": [
        "### Pre-example for Pyro.plate\n",
        "- Each invocation of plate requires the user to provide a unique name. The second argument is an integer just like for range.\n",
        "- Pyro can now leverage the conditional independency of the observations given the latent random variable. Basically pyro.plate is implemented using a context manager. At every execution of the body of the for loop we enter a new (conditional) independence context which is then exited at the end of the for loop body. \n",
        "  - because each observed pyro.sample statement occurs within a different execution of the body of the for loop, Pyro marks each observation as independent.\n",
        "  - this independence is properly a conditional independence given latent_fairness because latent_fairness is sampled outside of the context of data_loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK0eQFP3SiDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comparison between range() and pyro.plate\n",
        "def model(data):\n",
        "    # sample f from the beta prior\n",
        "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
        "    # loop over the observed data using pyro.sample with the obs keyword argument\n",
        "    for i in range(len(data)):\n",
        "        # observe datapoint i using the bernoulli likelihood\n",
        "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
        "        \n",
        "def model(data):\n",
        "    # sample f from the beta prior\n",
        "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
        "    # loop over the observed data [WE ONLY CHANGE THE NEXT LINE]\n",
        "    for i in pyro.plate(\"data_loop\", len(data)):\n",
        "        # observe datapoint i using the bernoulli likelihood\n",
        "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
        "\n",
        "# subsample minibatches of data\n",
        "with plate(\"data\", len(data), subsample_size=100) as ind:\n",
        "  batch = data[ind]\n",
        "  assert len(batch) == 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urMjXxsHinsL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "8128696e-39a6-4774-d48f-aea7692930c4"
      },
      "source": [
        "\"\"\"\n",
        "This example demonstrates the functionality of `pyro.contrib.minipyro`,\n",
        "which is a minimal implementation of the Pyro Probabilistic Programming\n",
        "Language that was created for didactic purposes.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import argparse\n",
        "import torch\n",
        "import pyro\n",
        "\n",
        "# We use the pyro.generic interface to support dynamic choice of backend.\n",
        "from pyro.generic import pyro_backend\n",
        "from pyro.generic import distributions as dist\n",
        "from pyro.generic import infer, optim, pyro\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # Define a basic model with a single Normal latent random variable `loc`\n",
        "    # and a batch of Normally distributed observations.\n",
        "    def model(data):\n",
        "        loc = pyro.sample(\"loc\", dist.Normal(0., 1.))\n",
        "        with pyro.plate(\"data\", len(data), dim=-1):\n",
        "            pyro.sample(\"obs\", dist.Normal(loc, 1.), obs=data)\n",
        "\n",
        "    # Define a guide (i.e. variational distribution) with a Normal\n",
        "    # distribution over the latent random variable `loc`.\n",
        "    def guide(data):\n",
        "        guide_loc = pyro.param(\"guide_loc\", torch.tensor(0.))\n",
        "        guide_scale = pyro.param(\"guide_scale_log\", torch.tensor(0.)).exp()\n",
        "        pyro.sample(\"loc\", dist.Normal(guide_loc, guide_scale))\n",
        "\n",
        "    # Generate some data.\n",
        "    torch.manual_seed(0)\n",
        "    data = torch.randn(100) + 3.0\n",
        "\n",
        "    # Because the API in minipyro matches that of Pyro proper,\n",
        "    # training code works with generic Pyro implementations.\n",
        "    with pyro_backend(args[\"backend\"]):\n",
        "        # Construct an SVI object so we can do variational inference on our\n",
        "        # model/guide pair.\n",
        "        elbo = infer.Trace_ELBO()\n",
        "        adam = optim.Adam({\"lr\": args[\"learning_rate\"]})\n",
        "        svi = infer.SVI(model, guide, adam, elbo)\n",
        "\n",
        "        # Basic training loop\n",
        "        pyro.get_param_store().clear()\n",
        "        for step in range(args[\"num_steps\"]):\n",
        "            loss = svi.step(data)\n",
        "            if step % 100 == 0:\n",
        "                print(\"step {} loss = {}\".format(step, loss))\n",
        "\n",
        "        # Report the final values of the variational parameters\n",
        "        # in the guide after training.\n",
        "        for name in pyro.get_param_store():\n",
        "            value = pyro.param(name)\n",
        "            print(\"{} = {}\".format(name, value.detach().cpu().numpy()))\n",
        "\n",
        "        # For this simple (conjugate) model we know the exact posterior. In\n",
        "        # particular we know that the variational distribution should be\n",
        "        # centered near 3.0. So let's check this explicitly.\n",
        "        assert (pyro.param(\"guide_loc\") - 3.0).abs() < 0.1\n",
        "\n",
        "args = {\"num_steps\": 1001, \"learning_rate\": 0.02, \"backend\": \"minipyro\"}\n",
        "main(args)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     assert pyro.__version__.startswith('0.3.3')\n",
        "#     parser = argparse.ArgumentParser(description=\"Mini Pyro demo\")\n",
        "#     parser.add_argument(\"-b\", \"--backend\", default=\"minipyro\")\n",
        "#     parser.add_argument(\"-n\", \"--num-steps\", default=1001, type=int)\n",
        "#     parser.add_argument(\"-lr\", \"--learning-rate\", default=0.02, type=float)\n",
        "#     args = parser.parse_args()\n",
        "#     main(args)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0 loss = 291.2471618652344\n",
            "step 100 loss = 164.15792846679688\n",
            "step 200 loss = 149.47970581054688\n",
            "step 300 loss = 150.03028869628906\n",
            "step 400 loss = 165.29713439941406\n",
            "step 500 loss = 153.3885955810547\n",
            "step 600 loss = 164.81736755371094\n",
            "step 700 loss = 150.8622589111328\n",
            "step 800 loss = 150.74578857421875\n",
            "step 900 loss = 150.77191162109375\n",
            "step 1000 loss = 152.4605712890625\n",
            "guide_loc = 3.0301661491394043\n",
            "guide_scale_log = -1.7844144105911255\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}