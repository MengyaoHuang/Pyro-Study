{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVI Part I: An Introduction to Stochastic Variational Inference in Pyro.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3AQi81q1Qt2",
        "colab_type": "text"
      },
      "source": [
        "### SVI Part I: An Introduction to Stochastic Variational Inference in Pyro\n",
        "- The different pieces of model() are encoded via the mapping:\n",
        "  - observations ⟺ pyro.sample with the obs argument\n",
        "  - latent random variables ⟺ pyro.sample\n",
        "  - parameters ⟺ pyro.param\n",
        "- Now let’s establish some notation. The model has observations x and latent random variables z as well as parameters θ. It has a joint probability density of the form: pθ(x,z)=pθ(x|z)pθ(z)\n",
        "  - We assume that the various probability distributions pi that make up pθ(x,z) have the following properties:\n",
        "    - we can sample from each pi\n",
        "    - we can compute the pointwise log pdf pi\n",
        "    - pi is differentiable w.r.t. the parameters θ\n",
        "  - Model Learning\n",
        "    - in this context our criterion for learning a good model will be maximizing the log evidence, i.e. we want to find the value of θ given by θmax=argmaxθlogpθ(x)\n",
        "    - Variational inference offers a scheme for finding θmax and computing an approximation to the posterior pθmax(z|x). Let’s see how that works.\n",
        "- (Guide function) The basic idea is that we introduce a parameterized distribution qϕ(z), where ϕ are known as the variational parameters. This distribution is called the variational distribution in much of the literature, and in the context of Pyro it’s called the guide (one syllable instead of nine!). The guide will serve as an approximation to the posterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0F4pSqR1TsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Do NOT run\n",
        "\"\"\"\n",
        "# random variables are specified in Pyro with primitive statement pyro.sample() \n",
        "# the first argument denotes the name of the random variable\n",
        "def model():\n",
        "    pyro.sample(\"z_1\", ...)\n",
        "# then the guide needs to have a matching sample statement\n",
        "def guide():\n",
        "    pyro.sample(\"z_1\", ...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB7Q-HC3Crk6",
        "colab_type": "text"
      },
      "source": [
        "- Learning will be setup as an optimization problem where each iteration of training takes a step in θ−ϕ space that moves the guide closer to the exact posterior. To do this we need to define an appropriate objective function\n",
        "\n",
        "### ELBO - the evidence lower bound\n",
        "- The ELBO, which is a function of both θ and ϕ, is defined as an expectation w.r.t. to samples from the guide: ELBO≡Eqϕ(z)[logpθ(x,z)−logqϕ(z)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOgfMdpbD-Kj",
        "colab_type": "text"
      },
      "source": [
        "### SVI Class\n",
        "- At present SVI only provides support for the ELBO objective\n",
        "- The user needs to provide three things: the model, the guide, and an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVPdklItCgzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyro\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "# The SVI object provides two methods, step(arg) and evaluate_loss(arg)\n",
        "# svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
        "\n",
        "# arguments used to instantiate PyTorch optimizers for all the parameters\n",
        "from pyro.optim import Adam\n",
        "adam_params = {\"lr\": 0.005, \"betas\": (0.95, 0.999)}\n",
        "optimizer = Adam(adam_params)\n",
        "\n",
        "# simple example to illustrate the API\n",
        "def per_param_callable(module_name, param_name):\n",
        "    if param_name == 'my_special_parameter':\n",
        "        return {\"lr\": 0.010}\n",
        "    else:\n",
        "        return {\"lr\": 0.001}\n",
        "optimizer = Adam(per_param_callable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFDkIlPcItXw",
        "colab_type": "text"
      },
      "source": [
        "### A simple example - fairness of a two-sided coin\n",
        "- we encode heads and tails as 1s and 0s. We encode the fairness of the coin as a real number f, where f satisfies f∈[0.0,1.0] and f=0.50 corresponds to a perfectly fair coin. \n",
        "- Our prior belief about f will be encoded by a beta distribution, specifically Beta(10,10), which is a symmetric probability distribution on the interval [0.0,1.0] that is peaked at f=0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-skV04nCgxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyro.distributions as dist\n",
        "\n",
        "def model(data):\n",
        "    # define the hyperparameters that control the beta prior\n",
        "    alpha0 = torch.tensor(10.0)\n",
        "    beta0 = torch.tensor(10.0)\n",
        "    # sample f from the beta prior\n",
        "    # a single latent random variable ('latent_fairness'), \n",
        "    # which is distributed according to Beta(10,10).\n",
        "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
        "    # loop over the observed data\n",
        "    for i in range(len(data)):\n",
        "        # observe datapoint i using the bernoulli\n",
        "        # likelihood Bernoulli(f)\n",
        "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd1FqKiaPS5-",
        "colab_type": "text"
      },
      "source": [
        "- Our next task is to define a corresponding guide, i.e. an appropriate variational distribution for the latent random variable f. The only real requirement here is that q(f) should be a probability distribution over the range [0.0,1.0], since f doesn’t make sense outside of that range. \n",
        "- A simple choice is to use another beta distribution parameterized by two trainable parameters αq and βq. Actually, in this particular case this is the ‘right’ choice, since conjugacy of the bernoulli and beta distributions means that the exact posterior is a beta distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGcPvCVfCgup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model(data) and guide(data) take the same arguments\n",
        "# use constraint=constraints.positive to ensure that \n",
        "# alpha_q and beta_q remain non-negative during optimization\n",
        "def guide(data):\n",
        "    # register the two variational parameters with Pyro.\n",
        "    # - both parameters will have initial value 15.0.\n",
        "    # - because we invoke constraints.positive, the optimizer\n",
        "    # will take gradients on the unconstrained parameters\n",
        "    # (which are related to the constrained parameters by a log)\n",
        "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0),\n",
        "                         constraint=constraints.positive)\n",
        "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0),\n",
        "                        constraint=constraints.positive)\n",
        "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
        "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbAm88npQX_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "361efffc-5496-4ed0-f51b-117ec60a85fc"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.distributions.constraints as constraints\n",
        "import pyro\n",
        "from pyro.optim import Adam\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "import pyro.distributions as dist\n",
        "\n",
        "# this is for running the notebook in our testing framework\n",
        "smoke_test = ('CI' in os.environ)\n",
        "n_steps = 2 if smoke_test else 2000\n",
        "\n",
        "# enable validation (e.g. validate parameters of distributions)\n",
        "pyro.enable_validation(True)\n",
        "\n",
        "# clear the param store in case we're in a REPL\n",
        "pyro.clear_param_store()\n",
        "\n",
        "# create some data with 6 observed heads and 4 observed tails\n",
        "data = []\n",
        "for _ in range(6):\n",
        "    data.append(torch.tensor(1.0))\n",
        "for _ in range(4):\n",
        "    data.append(torch.tensor(0.0))\n",
        "data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(1.),\n",
              " tensor(1.),\n",
              " tensor(1.),\n",
              " tensor(1.),\n",
              " tensor(1.),\n",
              " tensor(1.),\n",
              " tensor(0.),\n",
              " tensor(0.),\n",
              " tensor(0.),\n",
              " tensor(0.)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smcT2m55CgsX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "e2d107b4-f95e-4ea3-add8-dcb578c03b44"
      },
      "source": [
        "# setup the optimizer\n",
        "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
        "optimizer = Adam(adam_params)\n",
        "\n",
        "# setup the inference algorithm\n",
        "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
        "\n",
        "# do gradient steps\n",
        "for step in range(n_steps):\n",
        "  # in the step() method we pass in the data, \n",
        "  # which then get passed to the model and guide\n",
        "  svi.step(data)\n",
        "  if step % 100 == 0:\n",
        "    print('.', end='')\n",
        "\n",
        "# grab the learned variational parameters\n",
        "alpha_q = pyro.param(\"alpha_q\").item()\n",
        "beta_q = pyro.param(\"beta_q\").item()\n",
        "\n",
        "# here we use some facts about the beta distribution\n",
        "# compute the inferred mean of the coin's fairness\n",
        "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
        "# compute inferred standard deviation\n",
        "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
        "inferred_std = inferred_mean * math.sqrt(factor)\n",
        "\n",
        "print(\"\\nalpha_q value: \", alpha_q)\n",
        "print(\"beta_q value: \", beta_q)\n",
        "print(\"based on the data and our prior belief, the fairness \" +\n",
        "      \"of the coin is %.3f + - %.3f\" % (inferred_mean, inferred_std))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "..................................................\n",
            "alpha_q value:  16.174606323242188\n",
            "beta_q value:  14.03137493133545\n",
            "based on the data and our prior belief, the fairness of the coin is 0.535 + - 0.089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "583qYytwCgmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}